{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Intro","text":"<p><code>any-llm</code> is a Python library providing a single interface to different llm providers.</p>"},{"location":"#demo","title":"Demo","text":"<p>Try <code>any-llm</code> in action with our interactive chat demo that showcases streaming completions and provider switching:</p> <p>\ud83d\udcc2 Run the Demo</p> <p>The demo features real-time streaming responses, multiple provider support, and collapsible \"thinking\" content display.</p>"},{"location":"#getting-started","title":"Getting Started","text":"<p>Refer to the Quickstart for instructions on installation and usage.</p>"},{"location":"#api-documentation","title":"API Documentation","text":"<p><code>any-llm</code> provides two main interfaces:</p> <p>Direct API Functions (recommended for simple use cases): - completion - Chat completions with any provider - embedding - Text embeddings - responses - OpenAI-style Responses API</p> <p>AnyLLM Class (recommended for advanced use cases): - Provider API - Lower-level provider interface with metadata access and reusability</p>"},{"location":"#error-handling","title":"Error Handling","text":"<p><code>any-llm</code> provides custom exceptions to indicate common errors like missing API keys and parameters that are unsupported by a specific provider.</p> <p>For more details on exceptions, see the exceptions API documentation.</p>"},{"location":"#for-ai-systems","title":"For AI Systems","text":"<p>This documentation is available in two AI-friendly formats:</p> <ul> <li>llms.txt - A structured overview with curated links to key documentation sections</li> <li>llms-full.txt - Complete documentation content concatenated into a single file</li> </ul>"},{"location":"providers/","title":"Supported Providers","text":"<p><code>any-llm</code> supports the below providers. In order to discover information about what models are supported by a provider as well as what features the provider supports for each model, refer to the provider documentation.</p> <p>Provider source code can be found in the <code>src/any_llm/providers/</code> directory of the repository.</p> <p>Legend</p> <ul> <li>Reasoning (Completions): Provider can return reasoning traces alongside the assistant message via the completions and/or streaming endpoints. This does not indicate whether the provider offers separate \"reasoning models\".See this</li> <li>Streaming (Completions): Provider can stream completion results back as an iterator. discussion for more information.</li> <li>Image (Completions): Provider supports passing an <code>image_data</code> parameter for vision capabilities, as defined by the OpenAI spec here.</li> <li>Responses API: Provider supports the Responses API variant for text generation.  See this to follow along with our implementation effort.</li> <li>List Models API: Provider supports listing available models programmatically via the <code>list_models()</code> function. This allows you to discover what models are available from the provider at runtime, which can be useful for dynamic model selection or validation.</li> </ul> ID Env Var Responses Completion Streaming(Completions) Reasoning(Completions) Image (Completions) Embedding List Models Batch <code>anthropic</code> ANTHROPIC_API_KEY \u274c \u2705 \u2705 \u2705 \u2705 \u274c \u2705 \u274c <code>azure</code> AZURE_API_KEY \u274c \u2705 \u2705 \u274c \u274c \u2705 \u274c \u274c <code>azureopenai</code> AZURE_OPENAI_API_KEY \u2705 \u2705 \u2705 \u274c \u2705 \u2705 \u2705 \u274c <code>bedrock</code> AWS_BEARER_TOKEN_BEDROCK \u274c \u2705 \u2705 \u2705 \u274c \u2705 \u2705 \u274c <code>cerebras</code> CEREBRAS_API_KEY \u274c \u2705 \u2705 \u2705 \u274c \u274c \u2705 \u274c <code>cohere</code> COHERE_API_KEY \u274c \u2705 \u2705 \u2705 \u274c \u274c \u2705 \u274c <code>databricks</code> DATABRICKS_TOKEN \u274c \u2705 \u2705 \u2705 \u274c \u2705 \u274c \u274c <code>deepseek</code> DEEPSEEK_API_KEY \u274c \u2705 \u2705 \u2705 \u274c \u274c \u2705 \u274c <code>fireworks</code> FIREWORKS_API_KEY \u2705 \u2705 \u2705 \u2705 \u2705 \u274c \u2705 \u274c <code>gemini</code> GEMINI_API_KEY/GOOGLE_API_KEY \u274c \u2705 \u2705 \u2705 \u274c \u2705 \u2705 \u274c <code>groq</code> GROQ_API_KEY \u2705 \u2705 \u2705 \u2705 \u274c \u274c \u2705 \u274c <code>huggingface</code> HF_TOKEN \u274c \u2705 \u2705 \u2705 \u274c \u274c \u2705 \u274c <code>inception</code> INCEPTION_API_KEY \u274c \u2705 \u2705 \u274c \u274c \u274c \u2705 \u274c <code>llama</code> LLAMA_API_KEY \u274c \u2705 \u2705 \u274c \u274c \u274c \u2705 \u274c <code>llamacpp</code> None \u274c \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u274c <code>llamafile</code> None \u274c \u2705 \u274c \u2705 \u274c \u274c \u2705 \u274c <code>lmstudio</code> LM_STUDIO_API_KEY \u274c \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u274c <code>mistral</code> MISTRAL_API_KEY \u274c \u2705 \u2705 \u2705 \u274c \u2705 \u2705 \u274c <code>moonshot</code> MOONSHOT_API_KEY \u274c \u2705 \u2705 \u2705 \u274c \u274c \u2705 \u274c <code>nebius</code> NEBIUS_API_KEY \u274c \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u274c <code>ollama</code> None \u274c \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u274c <code>openai</code> OPENAI_API_KEY \u2705 \u2705 \u2705 \u274c \u2705 \u2705 \u2705 \u2705 <code>openrouter</code> OPENROUTER_API_KEY \u274c \u2705 \u2705 \u2705 \u2705 \u274c \u2705 \u274c <code>perplexity</code> PERPLEXITY_API_KEY \u274c \u2705 \u2705 \u274c \u2705 \u274c \u274c \u274c <code>portkey</code> PORTKEY_API_KEY \u274c \u2705 \u2705 \u2705 \u2705 \u274c \u2705 \u274c <code>sagemaker</code> AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY \u274c \u2705 \u2705 \u274c \u2705 \u2705 \u274c \u274c <code>sambanova</code> SAMBANOVA_API_KEY \u274c \u2705 \u2705 \u2705 \u2705 \u2705 \u2705 \u274c <code>together</code> TOGETHER_API_KEY \u274c \u2705 \u2705 \u2705 \u2705 \u274c \u274c \u274c <code>vertexai</code> \u274c \u2705 \u2705 \u2705 \u274c \u2705 \u2705 \u274c <code>voyage</code> VOYAGE_API_KEY \u274c \u274c \u274c \u274c \u274c \u2705 \u274c \u274c <code>watsonx</code> WATSONX_API_KEY \u274c \u2705 \u2705 \u274c \u2705 \u274c \u2705 \u274c <code>xai</code> XAI_API_KEY \u274c \u2705 \u2705 \u2705 \u274c \u274c \u2705 \u274c"},{"location":"quickstart/","title":"Quickstart","text":""},{"location":"quickstart/#quickstart","title":"Quickstart","text":""},{"location":"quickstart/#requirements","title":"Requirements","text":"<ul> <li>Python 3.11 or newer</li> <li>API_KEYS to access to whichever LLM you choose to use.</li> </ul>"},{"location":"quickstart/#installation","title":"Installation","text":""},{"location":"quickstart/#direct-usage","title":"Direct Usage","text":"<p>In your pip install, include the supported providers that you plan on using, or use the <code>all</code> option if you want to install support for all <code>any-llm</code> supported providers.</p> <pre><code>pip install any-llm-sdk[mistral]  # For Mistral provider\npip install any-llm-sdk[ollama]   # For Ollama provider\n# install multiple providers\npip install any-llm-sdk[mistral,ollama]\n# or install support for all providers\npip install any-llm-sdk[all]\n</code></pre>"},{"location":"quickstart/#library-integration","title":"Library Integration","text":"<p>If you're integrating <code>any-llm</code> into your own library that others will use, you only need to install the base package:</p> <pre><code>pip install any-llm-sdk\n</code></pre> <p>In this scenario, the end users of your library will be responsible for installing the appropriate provider dependencies when they want to use specific providers. <code>any-llm</code> is designed so that you'll only encounter exceptions at runtime if you try to use a provider without having the required dependencies installed.</p> <p>Those exceptions will clearly describe what needs to be installed to resolve the issue.</p> <p>Make sure you have the appropriate API key environment variable set for your provider. Alternatively, you could use the <code>api_key</code> parameter when making a completion call instead of setting an environment variable.</p> <pre><code>export MISTRAL_API_KEY=\"YOUR_KEY_HERE\"  # or OPENAI_API_KEY, etc\n</code></pre>"},{"location":"quickstart/#basic-usage","title":"Basic Usage","text":"<p><code>any-llm</code> provides two main approaches for working with LLM providers, each optimized for different use cases:</p>"},{"location":"quickstart/#option-1-direct-api-functions","title":"Option 1: Direct API Functions","text":"<p><code>completion</code> and <code>acompletion</code> provide a unified interface across all providers - perfect for simple use cases and quick prototyping.</p> <p>Recommended approach: Use separate <code>provider</code> and <code>model</code> parameters:</p> <pre><code>import os\n\nfrom any_llm import completion\n\n# Make sure you have the appropriate environment variable set\nassert os.environ.get('MISTRAL_API_KEY')\n\n# Recommended: separate provider and model parameters\nresponse = completion(\n    model=\"mistral-small-latest\",\n    provider=\"mistral\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\nprint(response.choices[0].message.content)\n</code></pre> <p>Alternative syntax: You can also use the combined <code>provider:model</code> format:</p> <pre><code>response = completion(\n    model=\"mistral:mistral-small-latest\",  # &lt;provider_id&gt;:&lt;model_id&gt;\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n</code></pre>"},{"location":"quickstart/#option-2-anyllm-class","title":"Option 2: AnyLLM Class","text":"<p>For advanced use cases that require provider reuse, metadata access, or more control over configuration:</p> <pre><code>import os\n\nfrom any_llm import AnyLLM\n\n# Make sure you have the appropriate environment variable set\nassert os.environ.get('MISTRAL_API_KEY')\n\nllm = AnyLLM.create(\"mistral\")\n\nresponse = llm.completion(\n    model=\"mistral-small-latest\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\nprint(response.choices[0].message.content)\n\nmetadata = llm.get_provider_metadata()\nprint(f\"Supports streaming: {metadata.streaming}\")\nprint(f\"Supports tools: {metadata.completion}\")\n</code></pre>"},{"location":"quickstart/#when-to-choose-which-approach","title":"When to Choose Which Approach","text":"<p>Use Direct API Functions (<code>completion</code>, <code>acompletion</code>) when:</p> <ul> <li>Making simple, one-off requests</li> <li>Prototyping or writing quick scripts</li> <li>You want the simplest possible interface</li> </ul> <p>Use Provider Class (<code>AnyLLM.create</code>) when:</p> <ul> <li>Building applications that make multiple requests with the same provider</li> <li>You want to avoid repeated provider instantiation overhead</li> </ul> <p>The provider_id should be specified according to the provider ids supported by any-llm. The <code>model_id</code> portion is passed directly to the provider internals: to understand what model ids are available for a provider, you will need to refer to the provider documentation or use our <code>list_models</code>  API if the provider supports that API.</p>"},{"location":"quickstart/#streaming","title":"Streaming","text":"<p>For the providers that support streaming, you can enable it by passing <code>stream=True</code>:</p> <pre><code>output = \"\"\nfor chunk in completion(\n    model=\"mistral-small-latest\",\n    provider=\"mistral\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}],\n    stream=True\n):\n    chunk_content = chunk.choices[0].delta.content or \"\"\n    print(chunk_content)\n    output += chunk_content\n</code></pre>"},{"location":"quickstart/#embeddings","title":"Embeddings","text":"<p><code>embedding</code> and <code>aembedding</code> allow you to create vector embeddings from text using the same unified interface across providers.</p> <p>Not all providers support embeddings - check the providers documentation to see which ones do.</p> <pre><code>from any_llm import embedding\n\nresult = embedding(\n    model=\"text-embedding-3-small\",\n    provider=\"openai\",\n    inputs=\"Hello, world!\" # can be either string or list of strings\n)\n\n# Access the embedding vector\nembedding_vector = result.data[0].embedding\nprint(f\"Embedding vector length: {len(embedding_vector)}\")\nprint(f\"Tokens used: {result.usage.total_tokens}\")\n</code></pre>"},{"location":"quickstart/#tools","title":"Tools","text":"<p><code>any-llm</code> supports tool calling for providers that support it. You can pass a list of tools where each tool is either:</p> <ol> <li>Python callable - Functions with proper docstrings and type annotations</li> <li>OpenAI Format tool dict - Already in OpenAI tool format</li> </ol> <pre><code>from any_llm import completion\n\ndef get_weather(location: str, unit: str = \"F\") -&gt; str:\n    \"\"\"Get weather information for a location.\n\n    Args:\n        location: The city or location to get weather for\n        unit: Temperature unit, either 'C' or 'F'\n    \"\"\"\n    return f\"Weather in {location} is sunny and 75{unit}!\"\n\nresponse = completion(\n    model=\"mistral-small-latest\",\n    provider=\"mistral\",\n    messages=[{\"role\": \"user\", \"content\": \"What's the weather in Pittsburgh PA?\"}],\n    tools=[get_weather]\n)\n</code></pre> <p>any-llm automatically converts your Python functions to OpenAI tools format. Functions must have: - A docstring describing what the function does - Type annotations for all parameters - A return type annotation</p>"},{"location":"api/any_llm/","title":"AnyLLM","text":""},{"location":"api/any_llm/#anyllm","title":"AnyLLM","text":""},{"location":"api/any_llm/#any_llm.AnyLLM","title":"<code>any_llm.AnyLLM</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Provider for the LLM.</p> Source code in <code>src/any_llm/any_llm.py</code> <pre><code>class AnyLLM(ABC):\n    \"\"\"Provider for the LLM.\"\"\"\n\n    # === Provider-specific configuration (to be overridden by subclasses) ===\n    PROVIDER_NAME: str\n    \"\"\"Must match the name of the provider directory  (case sensitive)\"\"\"\n\n    PROVIDER_DOCUMENTATION_URL: str\n    \"\"\"Link to the provider's documentation\"\"\"\n\n    ENV_API_KEY_NAME: str\n    \"\"\"Environment variable name for the API key\"\"\"\n\n    # === Feature support flags (to be set by subclasses) ===\n    SUPPORTS_COMPLETION_STREAMING: bool\n    \"\"\"OpenAI Streaming Completion API\"\"\"\n\n    SUPPORTS_COMPLETION: bool\n    \"\"\"OpenAI Completion API\"\"\"\n\n    SUPPORTS_COMPLETION_REASONING: bool\n    \"\"\"Reasoning Content attached to Completion API Response\"\"\"\n\n    SUPPORTS_COMPLETION_IMAGE: bool\n    \"\"\"Image Support for Completion API\"\"\"\n\n    SUPPORTS_COMPLETION_PDF: bool\n    \"\"\"PDF Support for Completion API\"\"\"\n\n    SUPPORTS_EMBEDDING: bool\n    \"\"\"OpenAI Embedding API\"\"\"\n\n    SUPPORTS_RESPONSES: bool\n    \"\"\"OpenAI Responses API\"\"\"\n\n    SUPPORTS_LIST_MODELS: bool\n    \"\"\"OpenAI Models API\"\"\"\n\n    SUPPORTS_BATCH: bool\n    \"\"\"OpenAI Batch Completion API\"\"\"\n\n    API_BASE: str | None = None\n    \"\"\"This is used to set the API base for the provider.\n    It is not required but may prove useful for providers that have overridable api bases.\n    \"\"\"\n\n    # === Internal Flag Checks ===\n    MISSING_PACKAGES_ERROR: ImportError | None = None\n    \"\"\"Some providers use SDKs that are not installed by default.\n    This flag is used to check if the packages are installed before instantiating the provider.\n    \"\"\"\n\n    BUILT_IN_TOOLS: ClassVar[list[Any] | None] = None\n    \"\"\"Some providers have built-in tools that can be used as-is without conversion.\n    This should be a list of the allowed built-in tool instances.\n    For example, in `gemini` provider, this could include `google.genai.types.Tool`.\n    \"\"\"\n\n    def __init__(self, api_key: str | None = None, api_base: str | None = None, **kwargs: Any) -&gt; None:\n        self._verify_no_missing_packages()\n        self._init_client(\n            api_key=self._verify_and_set_api_key(api_key),\n            api_base=api_base,\n            **kwargs,\n        )\n\n    def _verify_no_missing_packages(self) -&gt; None:\n        if self.MISSING_PACKAGES_ERROR is not None:\n            msg = f\"{self.PROVIDER_NAME} required packages are not installed. Please install them with `pip install any-llm-sdk[{self.PROVIDER_NAME}]`\"\n            raise ImportError(msg) from self.MISSING_PACKAGES_ERROR\n\n    def _verify_and_set_api_key(self, api_key: str | None = None) -&gt; str | None:\n        # Standardized API key handling. Splitting into its own function so that providers\n        # can easily override this method if they don't want verification (for instance, LMStudio)\n        if not api_key:\n            api_key = os.getenv(self.ENV_API_KEY_NAME)\n\n        if not api_key:\n            raise MissingApiKeyError(self.PROVIDER_NAME, self.ENV_API_KEY_NAME)\n        return api_key\n\n    @classmethod\n    def create(\n        cls, provider: str | LLMProvider, api_key: str | None = None, api_base: str | None = None, **kwargs: Any\n    ) -&gt; AnyLLM:\n        \"\"\"Create a provider instance using the given provider name and config.\n\n        Args:\n            provider: The provider name (e.g., 'openai', 'anthropic')\n            api_key: API key for the provider\n            api_base: Base URL for the provider API\n            **kwargs: Additional provider-specific arguments\n\n        Returns:\n            Provider instance for the specified provider\n\n        \"\"\"\n        return cls._create_provider(provider, api_key=api_key, api_base=api_base, **kwargs)\n\n    @classmethod\n    def _create_provider(\n        cls, provider_key: str | LLMProvider, api_key: str | None = None, api_base: str | None = None, **kwargs: Any\n    ) -&gt; AnyLLM:\n        \"\"\"Dynamically load and create an instance of a provider based on the naming convention.\"\"\"\n        provider_key = LLMProvider.from_string(provider_key).value\n\n        provider_class_name = f\"{provider_key.capitalize()}Provider\"\n        provider_module_name = f\"{provider_key}\"\n\n        module_path = f\"any_llm.providers.{provider_module_name}\"\n\n        try:\n            module = importlib.import_module(module_path)\n        except ImportError as e:\n            msg = f\"Could not import module {module_path}: {e!s}. Please ensure the provider is supported by doing AnyLLM.get_supported_providers()\"\n            raise ImportError(msg) from e\n\n        provider_class: type[AnyLLM] = getattr(module, provider_class_name)\n        return provider_class(api_key=api_key, api_base=api_base, **kwargs)\n\n    @classmethod\n    def get_provider_class(cls, provider_key: str | LLMProvider) -&gt; type[AnyLLM]:\n        \"\"\"Get the provider class without instantiating it.\n\n        Args:\n            provider_key: The provider key (e.g., 'anthropic', 'openai')\n\n        Returns:\n            The provider class\n\n        \"\"\"\n        provider_key = LLMProvider.from_string(provider_key).value\n\n        provider_class_name = f\"{provider_key.capitalize()}Provider\"\n        provider_module_name = f\"{provider_key}\"\n\n        module_path = f\"any_llm.providers.{provider_module_name}\"\n\n        try:\n            module = importlib.import_module(module_path)\n        except ImportError as e:\n            msg = f\"Could not import module {module_path}: {e!s}. Please ensure the provider is supported by doing AnyLLM.get_supported_providers()\"\n            raise ImportError(msg) from e\n\n        provider_class: type[AnyLLM] = getattr(module, provider_class_name)\n        return provider_class\n\n    @classmethod\n    def get_supported_providers(cls) -&gt; list[str]:\n        \"\"\"Get a list of supported provider keys.\"\"\"\n        return [provider.value for provider in LLMProvider]\n\n    @classmethod\n    def get_all_provider_metadata(cls) -&gt; list[ProviderMetadata]:\n        \"\"\"Get metadata for all supported providers.\n\n        Returns:\n            List of dictionaries containing provider metadata\n\n        \"\"\"\n        providers: list[ProviderMetadata] = []\n        for provider_key in cls.get_supported_providers():\n            provider_class = cls.get_provider_class(provider_key)\n            metadata = provider_class.get_provider_metadata()\n            providers.append(metadata)\n\n        # Sort providers by name\n        providers.sort(key=lambda x: x.name)\n        return providers\n\n    @classmethod\n    def get_provider_enum(cls, provider_key: str) -&gt; LLMProvider:\n        \"\"\"Convert a string provider key to a ProviderName enum.\"\"\"\n        try:\n            return LLMProvider(provider_key)\n        except ValueError as e:\n            supported = [provider.value for provider in LLMProvider]\n            raise UnsupportedProviderError(provider_key, supported) from e\n\n    @classmethod\n    def split_model_provider(cls, model: str) -&gt; tuple[LLMProvider, str]:\n        \"\"\"Extract the provider key from the model identifier.\n\n        Supports both new format 'provider:model' (e.g., 'mistral:mistral-small')\n        and legacy format 'provider/model' (e.g., 'mistral/mistral-small').\n\n        The legacy format will be deprecated in version 1.0.\n        \"\"\"\n        colon_index = model.find(\":\")\n        slash_index = model.find(\"/\")\n\n        # Determine which delimiter comes first\n        if colon_index != -1 and (slash_index == -1 or colon_index &lt; slash_index):\n            # The colon came first, so it's using the new syntax.\n            provider, model_name = model.split(\":\", 1)\n        elif slash_index != -1:\n            # Slash comes first, so it's the legacy syntax\n            warnings.warn(\n                f\"Model format 'provider/model' is deprecated and will be removed in version 1.0. \"\n                f\"Please use 'provider:model' format instead. Got: '{model}'\",\n                DeprecationWarning,\n                stacklevel=3,\n            )\n            provider, model_name = model.split(\"/\", 1)\n        else:\n            msg = f\"Invalid model format. Expected 'provider:model' or 'provider/model', got '{model}'\"\n            raise ValueError(msg)\n\n        if not provider or not model_name:\n            msg = f\"Invalid model format. Expected 'provider:model' or 'provider/model', got '{model}'\"\n            raise ValueError(msg)\n        return cls.get_provider_enum(provider), model_name\n\n    @staticmethod\n    @abstractmethod\n    def _convert_completion_params(params: CompletionParams, **kwargs: Any) -&gt; dict[str, Any]:\n        msg = \"Subclasses must implement this method\"\n        raise NotImplementedError(msg)\n\n    @staticmethod\n    @abstractmethod\n    def _convert_completion_response(response: Any) -&gt; ChatCompletion:\n        msg = \"Subclasses must implement this method\"\n        raise NotImplementedError(msg)\n\n    @staticmethod\n    @abstractmethod\n    def _convert_completion_chunk_response(response: Any, **kwargs: Any) -&gt; ChatCompletionChunk:\n        msg = \"Subclasses must implement this method\"\n        raise NotImplementedError(msg)\n\n    @staticmethod\n    @abstractmethod\n    def _convert_embedding_params(params: Any, **kwargs: Any) -&gt; dict[str, Any]:\n        msg = \"Subclasses must implement this method\"\n        raise NotImplementedError(msg)\n\n    @staticmethod\n    @abstractmethod\n    def _convert_embedding_response(response: Any) -&gt; CreateEmbeddingResponse:\n        msg = \"Subclasses must implement this method\"\n        raise NotImplementedError(msg)\n\n    @staticmethod\n    @abstractmethod\n    def _convert_list_models_response(response: Any) -&gt; Sequence[Model]:\n        msg = \"Subclasses must implement this method\"\n        raise NotImplementedError(msg)\n\n    @classmethod\n    def get_provider_metadata(cls) -&gt; ProviderMetadata:\n        \"\"\"Get provider metadata without requiring instantiation.\n\n        Returns:\n            Dictionary containing provider metadata including name, environment variable,\n            documentation URL, and class name.\n\n        \"\"\"\n        return ProviderMetadata(\n            name=cls.PROVIDER_NAME,\n            env_key=cls.ENV_API_KEY_NAME,\n            doc_url=cls.PROVIDER_DOCUMENTATION_URL,\n            streaming=cls.SUPPORTS_COMPLETION_STREAMING,\n            reasoning=cls.SUPPORTS_COMPLETION_REASONING,\n            completion=cls.SUPPORTS_COMPLETION,\n            image=cls.SUPPORTS_COMPLETION_IMAGE,\n            pdf=cls.SUPPORTS_COMPLETION_PDF,\n            embedding=cls.SUPPORTS_EMBEDDING,\n            responses=cls.SUPPORTS_RESPONSES,\n            list_models=cls.SUPPORTS_LIST_MODELS,\n            batch_completion=cls.SUPPORTS_BATCH,\n            class_name=cls.__name__,\n        )\n\n    @abstractmethod\n    def _init_client(self, api_key: str | None = None, api_base: str | None = None, **kwargs: Any) -&gt; None:\n        msg = \"Subclasses must implement this method\"\n        raise NotImplementedError(msg)\n\n    def completion(\n        self,\n        **kwargs: Any,\n    ) -&gt; ChatCompletion | Iterator[ChatCompletionChunk]:\n        \"\"\"Create a chat completion synchronously.\n\n        See [AnyLLM.acompletion][any_llm.any_llm.AnyLLM.acompletion]\n        \"\"\"\n        allow_running_loop = kwargs.pop(\"allow_running_loop\", INSIDE_NOTEBOOK)\n        response = run_async_in_sync(self.acompletion(**kwargs), allow_running_loop=allow_running_loop)\n        if isinstance(response, ChatCompletion):\n            return response\n\n        return async_iter_to_sync_iter(response)\n\n    async def acompletion(\n        self,\n        model: str,\n        messages: list[dict[str, Any] | ChatCompletionMessage],\n        *,\n        tools: list[dict[str, Any] | Callable[..., Any]] | Any | None = None,\n        tool_choice: str | dict[str, Any] | None = None,\n        temperature: float | None = None,\n        top_p: float | None = None,\n        max_tokens: int | None = None,\n        response_format: dict[str, Any] | type[BaseModel] | None = None,\n        stream: bool | None = None,\n        n: int | None = None,\n        stop: str | list[str] | None = None,\n        presence_penalty: float | None = None,\n        frequency_penalty: float | None = None,\n        seed: int | None = None,\n        user: str | None = None,\n        parallel_tool_calls: bool | None = None,\n        logprobs: bool | None = None,\n        top_logprobs: int | None = None,\n        logit_bias: dict[str, float] | None = None,\n        stream_options: dict[str, Any] | None = None,\n        max_completion_tokens: int | None = None,\n        reasoning_effort: Literal[\"minimal\", \"low\", \"medium\", \"high\", \"auto\"] | None = \"auto\",\n        **kwargs: Any,\n    ) -&gt; ChatCompletion | AsyncIterator[ChatCompletionChunk]:\n        \"\"\"Create a chat completion asynchronously.\n\n        Args:\n            model: Model identifier for the chosen provider (e.g., model='gpt-4.1-mini' for LLMProvider.OPENAI).\n            messages: List of messages for the conversation\n            tools: List of tools for tool calling. Can be Python callables or OpenAI tool format dicts\n            tool_choice: Controls which tools the model can call\n            temperature: Controls randomness in the response (0.0 to 2.0)\n            top_p: Controls diversity via nucleus sampling (0.0 to 1.0)\n            max_tokens: Maximum number of tokens to generate\n            response_format: Format specification for the response\n            stream: Whether to stream the response\n            n: Number of completions to generate\n            stop: Stop sequences for generation\n            presence_penalty: Penalize new tokens based on presence in text\n            frequency_penalty: Penalize new tokens based on frequency in text\n            seed: Random seed for reproducible results\n            user: Unique identifier for the end user\n            parallel_tool_calls: Whether to allow parallel tool calls\n            logprobs: Include token-level log probabilities in the response\n            top_logprobs: Number of alternatives to return when logprobs are requested\n            logit_bias: Bias the likelihood of specified tokens during generation\n            stream_options: Additional options controlling streaming behavior\n            max_completion_tokens: Maximum number of tokens for the completion\n            reasoning_effort: Reasoning effort level for models that support it. \"auto\" will map to each provider's default.\n            **kwargs: Additional provider-specific arguments that will be passed to the provider's API call.\n\n        Returns:\n            The completion response from the provider\n\n        \"\"\"\n        all_args = locals()\n        all_args.pop(\"self\")\n        all_args[\"model_id\"] = all_args.pop(\"model\")\n        kwargs = all_args.pop(\"kwargs\")\n\n        if tools:\n            all_args[\"tools\"] = prepare_tools(tools, built_in_tools=self.BUILT_IN_TOOLS)\n\n        for i, message in enumerate(messages):\n            if isinstance(message, ChatCompletionMessage):\n                # Dump the message but exclude the extra field that we extend from OpenAI Spec\n                messages[i] = message.model_dump(exclude_none=True, exclude={\"reasoning\"})\n        all_args[\"messages\"] = messages\n\n        return await self._acompletion(CompletionParams(**all_args), **kwargs)\n\n    async def _acompletion(\n        self, params: CompletionParams, **kwargs: Any\n    ) -&gt; ChatCompletion | AsyncIterator[ChatCompletionChunk]:\n        if not self.SUPPORTS_COMPLETION:\n            msg = \"Provider doesn't support completion.\"\n            raise NotImplementedError(msg)\n        msg = \"Subclasses must implement _acompletion method\"\n        raise NotImplementedError(msg)\n\n    def responses(self, **kwargs: Any) -&gt; Response | Iterator[ResponseStreamEvent]:\n        \"\"\"Create a response synchronously.\n\n        See [AnyLLM.aresponses][any_llm.any_llm.AnyLLM.aresponses]\n        \"\"\"\n        allow_running_loop = kwargs.pop(\"allow_running_loop\", INSIDE_NOTEBOOK)\n        response = run_async_in_sync(self.aresponses(**kwargs), allow_running_loop=allow_running_loop)\n        if isinstance(response, Response):\n            return response\n        return async_iter_to_sync_iter(response)\n\n    async def aresponses(\n        self,\n        model: str,\n        input_data: str | ResponseInputParam,\n        *,\n        tools: list[dict[str, Any] | Callable[..., Any]] | Any | None = None,\n        tool_choice: str | dict[str, Any] | None = None,\n        max_output_tokens: int | None = None,\n        temperature: float | None = None,\n        top_p: float | None = None,\n        stream: bool | None = None,\n        instructions: str | None = None,\n        max_tool_calls: int | None = None,\n        parallel_tool_calls: int | None = None,\n        reasoning: Any | None = None,\n        text: Any | None = None,\n        **kwargs: Any,\n    ) -&gt; Response | AsyncIterator[ResponseStreamEvent]:\n        \"\"\"Create a response using the OpenAI-style Responses API.\n\n        This follows the OpenAI Responses API shape and returns the aliased\n        `any_llm.types.responses.Response` type. If `stream=True`, an iterator of\n        `any_llm.types.responses.ResponseStreamEvent` items is returned.\n\n        Args:\n            model: Model identifier for the chosen provider (e.g., model='gpt-4.1-mini' for LLMProvider.OPENAI).\n            input_data: The input payload accepted by provider's Responses API.\n                For OpenAI-compatible providers, this is typically a list mixing\n                text, images, and tool instructions, or a dict per OpenAI spec.\n            tools: Optional tools for tool calling (Python callables or OpenAI tool dicts)\n            tool_choice: Controls which tools the model can call\n            max_output_tokens: Maximum number of output tokens to generate\n            temperature: Controls randomness in the response (0.0 to 2.0)\n            top_p: Controls diversity via nucleus sampling (0.0 to 1.0)\n            stream: Whether to stream response events\n            instructions: A system (or developer) message inserted into the model's context.\n            max_tool_calls: The maximum number of total calls to built-in tools that can be processed in a response. This maximum number applies across all built-in tool calls, not per individual tool. Any further attempts to call a tool by the model will be ignored.\n            parallel_tool_calls: Whether to allow the model to run tool calls in parallel.\n            reasoning: Configuration options for reasoning models.\n            text: Configuration options for a text response from the model. Can be plain text or structured JSON data.\n            **kwargs: Additional provider-specific arguments that will be passed to the provider's API call.\n\n        Returns:\n            Either a `Response` object (non-streaming) or an iterator of\n            `ResponseStreamEvent` (streaming).\n\n        Raises:\n            NotImplementedError: If the selected provider does not support the Responses API.\n\n        \"\"\"\n        all_args = locals()\n        all_args.pop(\"self\")\n        all_args[\"input\"] = all_args.pop(\"input_data\")\n        kwargs = all_args.pop(\"kwargs\")\n\n        if tools:\n            all_args[\"tools\"] = prepare_tools(tools, built_in_tools=self.BUILT_IN_TOOLS)\n\n        return await self._aresponses(ResponsesParams(**all_args, **kwargs))\n\n    async def _aresponses(\n        self, params: ResponsesParams, **kwargs: Any\n    ) -&gt; Response | AsyncIterator[ResponseStreamEvent]:\n        if not self.SUPPORTS_RESPONSES:\n            msg = \"Provider doesn't support responses.\"\n            raise NotImplementedError(msg)\n        msg = \"Subclasses must implement _aresponses method\"\n        raise NotImplementedError(msg)\n\n    def _embedding(self, model: str, inputs: str | list[str], **kwargs: Any) -&gt; CreateEmbeddingResponse:\n        allow_running_loop = kwargs.pop(\"allow_running_loop\", INSIDE_NOTEBOOK)\n        return run_async_in_sync(self.aembedding(model, inputs, **kwargs), allow_running_loop=allow_running_loop)\n\n    async def aembedding(self, model: str, inputs: str | list[str], **kwargs: Any) -&gt; CreateEmbeddingResponse:\n        return await self._aembedding(model, inputs, **kwargs)\n\n    async def _aembedding(self, model: str, inputs: str | list[str], **kwargs: Any) -&gt; CreateEmbeddingResponse:\n        if not self.SUPPORTS_EMBEDDING:\n            msg = \"Provider doesn't support embedding.\"\n            raise NotImplementedError(msg)\n        msg = \"Subclasses must implement _aembedding method\"\n        raise NotImplementedError(msg)\n\n    def list_models(self, **kwargs: Any) -&gt; Sequence[Model]:\n        allow_running_loop = kwargs.pop(\"allow_running_loop\", INSIDE_NOTEBOOK)\n        return run_async_in_sync(self.alist_models(**kwargs), allow_running_loop=allow_running_loop)\n\n    async def alist_models(self, **kwargs: Any) -&gt; Sequence[Model]:\n        return await self._alist_models(**kwargs)\n\n    async def _alist_models(self, **kwargs: Any) -&gt; Sequence[Model]:\n        if not self.SUPPORTS_LIST_MODELS:\n            msg = \"Provider doesn't support listing models.\"\n            raise NotImplementedError(msg)\n        msg = \"Subclasses must implement _alist_models method\"\n        raise NotImplementedError(msg)\n\n    @experimental(BATCH_API_EXPERIMENTAL_MESSAGE)\n    def create_batch(self, **kwargs: Any) -&gt; Batch:\n        \"\"\"Create a batch synchronously.\n\n        See [AnyLLM.acreate_batch][any_llm.any_llm.AnyLLM.acreate_batch]\n        \"\"\"\n        allow_running_loop = kwargs.pop(\"allow_running_loop\", INSIDE_NOTEBOOK)\n        return run_async_in_sync(self.acreate_batch(**kwargs), allow_running_loop=allow_running_loop)\n\n    @experimental(BATCH_API_EXPERIMENTAL_MESSAGE)\n    async def acreate_batch(\n        self,\n        input_file_path: str,\n        endpoint: str,\n        completion_window: str = \"24h\",\n        metadata: dict[str, str] | None = None,\n        **kwargs: Any,\n    ) -&gt; Batch:\n        \"\"\"Create a batch job asynchronously.\n\n        Args:\n            input_file_path: Path to a local file containing batch requests in JSONL format.\n            endpoint: The endpoint to be used for all requests (e.g., '/v1/chat/completions')\n            completion_window: The time frame within which the batch should be processed (default: '24h')\n            metadata: Optional custom metadata for the batch\n            **kwargs: Additional provider-specific arguments\n\n        Returns:\n            The created batch object\n\n        \"\"\"\n        return await self._acreate_batch(\n            input_file_path=input_file_path,\n            endpoint=endpoint,\n            completion_window=completion_window,\n            metadata=metadata,\n            **kwargs,\n        )\n\n    async def _acreate_batch(\n        self,\n        input_file_path: str,\n        endpoint: str,\n        completion_window: str = \"24h\",\n        metadata: dict[str, str] | None = None,\n        **kwargs: Any,\n    ) -&gt; Batch:\n        if not self.SUPPORTS_BATCH:\n            msg = \"Provider doesn't support batch completions.\"\n            raise NotImplementedError(msg)\n        msg = \"Subclasses must implement _acreate_batch method\"\n        raise NotImplementedError(msg)\n\n    @experimental(BATCH_API_EXPERIMENTAL_MESSAGE)\n    def retrieve_batch(self, batch_id: str, **kwargs: Any) -&gt; Batch:\n        \"\"\"Retrieve a batch synchronously.\n\n        See [AnyLLM.aretrieve_batch][any_llm.any_llm.AnyLLM.aretrieve_batch]\n        \"\"\"\n        allow_running_loop = kwargs.pop(\"allow_running_loop\", INSIDE_NOTEBOOK)\n        return run_async_in_sync(self.aretrieve_batch(batch_id, **kwargs), allow_running_loop=allow_running_loop)\n\n    @experimental(BATCH_API_EXPERIMENTAL_MESSAGE)\n    async def aretrieve_batch(self, batch_id: str, **kwargs: Any) -&gt; Batch:\n        \"\"\"Retrieve a batch job asynchronously.\n\n        Args:\n            batch_id: The ID of the batch to retrieve\n            **kwargs: Additional provider-specific arguments\n\n        Returns:\n            The batch object\n\n        \"\"\"\n        return await self._aretrieve_batch(batch_id, **kwargs)\n\n    async def _aretrieve_batch(self, batch_id: str, **kwargs: Any) -&gt; Batch:\n        if not self.SUPPORTS_BATCH:\n            msg = \"Provider doesn't support batch completions.\"\n            raise NotImplementedError(msg)\n        msg = \"Subclasses must implement _aretrieve_batch method\"\n        raise NotImplementedError(msg)\n\n    @experimental(BATCH_API_EXPERIMENTAL_MESSAGE)\n    def cancel_batch(self, batch_id: str, **kwargs: Any) -&gt; Batch:\n        \"\"\"Cancel a batch synchronously.\n\n        See [AnyLLM.acancel_batch][any_llm.any_llm.AnyLLM.acancel_batch]\n        \"\"\"\n        allow_running_loop = kwargs.pop(\"allow_running_loop\", INSIDE_NOTEBOOK)\n        return run_async_in_sync(self.acancel_batch(batch_id, **kwargs), allow_running_loop=allow_running_loop)\n\n    @experimental(BATCH_API_EXPERIMENTAL_MESSAGE)\n    async def acancel_batch(self, batch_id: str, **kwargs: Any) -&gt; Batch:\n        \"\"\"Cancel a batch job asynchronously.\n\n        Args:\n            batch_id: The ID of the batch to cancel\n            **kwargs: Additional provider-specific arguments\n\n        Returns:\n            The cancelled batch object\n\n        \"\"\"\n        return await self._acancel_batch(batch_id, **kwargs)\n\n    async def _acancel_batch(self, batch_id: str, **kwargs: Any) -&gt; Batch:\n        if not self.SUPPORTS_BATCH:\n            msg = \"Provider doesn't support batch completions.\"\n            raise NotImplementedError(msg)\n        msg = \"Subclasses must implement _acancel_batch method\"\n        raise NotImplementedError(msg)\n\n    @experimental(BATCH_API_EXPERIMENTAL_MESSAGE)\n    def list_batches(\n        self,\n        after: str | None = None,\n        limit: int | None = None,\n        **kwargs: Any,\n    ) -&gt; Sequence[Batch]:\n        \"\"\"List batches synchronously.\n\n        See [AnyLLM.alist_batches][any_llm.any_llm.AnyLLM.alist_batches]\n        \"\"\"\n        allow_running_loop = kwargs.pop(\"allow_running_loop\", INSIDE_NOTEBOOK)\n        return run_async_in_sync(\n            self.alist_batches(after=after, limit=limit, **kwargs), allow_running_loop=allow_running_loop\n        )\n\n    @experimental(BATCH_API_EXPERIMENTAL_MESSAGE)\n    async def alist_batches(\n        self,\n        after: str | None = None,\n        limit: int | None = None,\n        **kwargs: Any,\n    ) -&gt; Sequence[Batch]:\n        \"\"\"List batch jobs asynchronously.\n\n        Args:\n            after: A cursor for pagination. Returns batches after this batch ID.\n            limit: Maximum number of batches to return (default: 20)\n            **kwargs: Additional provider-specific arguments\n\n        Returns:\n            A list of batch objects\n\n        \"\"\"\n        return await self._alist_batches(after=after, limit=limit, **kwargs)\n\n    async def _alist_batches(\n        self,\n        after: str | None = None,\n        limit: int | None = None,\n        **kwargs: Any,\n    ) -&gt; Sequence[Batch]:\n        if not self.SUPPORTS_BATCH:\n            msg = \"Provider doesn't support batch completions.\"\n            raise NotImplementedError(msg)\n        msg = \"Subclasses must implement _alist_batches method\"\n        raise NotImplementedError(msg)\n</code></pre>"},{"location":"api/any_llm/#any_llm.AnyLLM.API_BASE","title":"<code>API_BASE = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>This is used to set the API base for the provider. It is not required but may prove useful for providers that have overridable api bases.</p>"},{"location":"api/any_llm/#any_llm.AnyLLM.BUILT_IN_TOOLS","title":"<code>BUILT_IN_TOOLS = None</code>  <code>class-attribute</code>","text":"<p>Some providers have built-in tools that can be used as-is without conversion. This should be a list of the allowed built-in tool instances. For example, in <code>gemini</code> provider, this could include <code>google.genai.types.Tool</code>.</p>"},{"location":"api/any_llm/#any_llm.AnyLLM.ENV_API_KEY_NAME","title":"<code>ENV_API_KEY_NAME</code>  <code>instance-attribute</code>","text":"<p>Environment variable name for the API key</p>"},{"location":"api/any_llm/#any_llm.AnyLLM.MISSING_PACKAGES_ERROR","title":"<code>MISSING_PACKAGES_ERROR = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Some providers use SDKs that are not installed by default. This flag is used to check if the packages are installed before instantiating the provider.</p>"},{"location":"api/any_llm/#any_llm.AnyLLM.PROVIDER_DOCUMENTATION_URL","title":"<code>PROVIDER_DOCUMENTATION_URL</code>  <code>instance-attribute</code>","text":"<p>Link to the provider's documentation</p>"},{"location":"api/any_llm/#any_llm.AnyLLM.PROVIDER_NAME","title":"<code>PROVIDER_NAME</code>  <code>instance-attribute</code>","text":"<p>Must match the name of the provider directory  (case sensitive)</p>"},{"location":"api/any_llm/#any_llm.AnyLLM.SUPPORTS_BATCH","title":"<code>SUPPORTS_BATCH</code>  <code>instance-attribute</code>","text":"<p>OpenAI Batch Completion API</p>"},{"location":"api/any_llm/#any_llm.AnyLLM.SUPPORTS_COMPLETION","title":"<code>SUPPORTS_COMPLETION</code>  <code>instance-attribute</code>","text":"<p>OpenAI Completion API</p>"},{"location":"api/any_llm/#any_llm.AnyLLM.SUPPORTS_COMPLETION_IMAGE","title":"<code>SUPPORTS_COMPLETION_IMAGE</code>  <code>instance-attribute</code>","text":"<p>Image Support for Completion API</p>"},{"location":"api/any_llm/#any_llm.AnyLLM.SUPPORTS_COMPLETION_PDF","title":"<code>SUPPORTS_COMPLETION_PDF</code>  <code>instance-attribute</code>","text":"<p>PDF Support for Completion API</p>"},{"location":"api/any_llm/#any_llm.AnyLLM.SUPPORTS_COMPLETION_REASONING","title":"<code>SUPPORTS_COMPLETION_REASONING</code>  <code>instance-attribute</code>","text":"<p>Reasoning Content attached to Completion API Response</p>"},{"location":"api/any_llm/#any_llm.AnyLLM.SUPPORTS_COMPLETION_STREAMING","title":"<code>SUPPORTS_COMPLETION_STREAMING</code>  <code>instance-attribute</code>","text":"<p>OpenAI Streaming Completion API</p>"},{"location":"api/any_llm/#any_llm.AnyLLM.SUPPORTS_EMBEDDING","title":"<code>SUPPORTS_EMBEDDING</code>  <code>instance-attribute</code>","text":"<p>OpenAI Embedding API</p>"},{"location":"api/any_llm/#any_llm.AnyLLM.SUPPORTS_LIST_MODELS","title":"<code>SUPPORTS_LIST_MODELS</code>  <code>instance-attribute</code>","text":"<p>OpenAI Models API</p>"},{"location":"api/any_llm/#any_llm.AnyLLM.SUPPORTS_RESPONSES","title":"<code>SUPPORTS_RESPONSES</code>  <code>instance-attribute</code>","text":"<p>OpenAI Responses API</p>"},{"location":"api/any_llm/#any_llm.AnyLLM.acancel_batch","title":"<code>acancel_batch(batch_id, **kwargs)</code>  <code>async</code>","text":"<p>Cancel a batch job asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>batch_id</code> <code>str</code> <p>The ID of the batch to cancel</p> required <code>**kwargs</code> <code>Any</code> <p>Additional provider-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Batch</code> <p>The cancelled batch object</p> Source code in <code>src/any_llm/any_llm.py</code> <pre><code>@experimental(BATCH_API_EXPERIMENTAL_MESSAGE)\nasync def acancel_batch(self, batch_id: str, **kwargs: Any) -&gt; Batch:\n    \"\"\"Cancel a batch job asynchronously.\n\n    Args:\n        batch_id: The ID of the batch to cancel\n        **kwargs: Additional provider-specific arguments\n\n    Returns:\n        The cancelled batch object\n\n    \"\"\"\n    return await self._acancel_batch(batch_id, **kwargs)\n</code></pre>"},{"location":"api/any_llm/#any_llm.AnyLLM.acompletion","title":"<code>acompletion(model, messages, *, tools=None, tool_choice=None, temperature=None, top_p=None, max_tokens=None, response_format=None, stream=None, n=None, stop=None, presence_penalty=None, frequency_penalty=None, seed=None, user=None, parallel_tool_calls=None, logprobs=None, top_logprobs=None, logit_bias=None, stream_options=None, max_completion_tokens=None, reasoning_effort='auto', **kwargs)</code>  <code>async</code>","text":"<p>Create a chat completion asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Model identifier for the chosen provider (e.g., model='gpt-4.1-mini' for LLMProvider.OPENAI).</p> required <code>messages</code> <code>list[dict[str, Any] | ChatCompletionMessage]</code> <p>List of messages for the conversation</p> required <code>tools</code> <code>list[dict[str, Any] | Callable[..., Any]] | Any | None</code> <p>List of tools for tool calling. Can be Python callables or OpenAI tool format dicts</p> <code>None</code> <code>tool_choice</code> <code>str | dict[str, Any] | None</code> <p>Controls which tools the model can call</p> <code>None</code> <code>temperature</code> <code>float | None</code> <p>Controls randomness in the response (0.0 to 2.0)</p> <code>None</code> <code>top_p</code> <code>float | None</code> <p>Controls diversity via nucleus sampling (0.0 to 1.0)</p> <code>None</code> <code>max_tokens</code> <code>int | None</code> <p>Maximum number of tokens to generate</p> <code>None</code> <code>response_format</code> <code>dict[str, Any] | type[BaseModel] | None</code> <p>Format specification for the response</p> <code>None</code> <code>stream</code> <code>bool | None</code> <p>Whether to stream the response</p> <code>None</code> <code>n</code> <code>int | None</code> <p>Number of completions to generate</p> <code>None</code> <code>stop</code> <code>str | list[str] | None</code> <p>Stop sequences for generation</p> <code>None</code> <code>presence_penalty</code> <code>float | None</code> <p>Penalize new tokens based on presence in text</p> <code>None</code> <code>frequency_penalty</code> <code>float | None</code> <p>Penalize new tokens based on frequency in text</p> <code>None</code> <code>seed</code> <code>int | None</code> <p>Random seed for reproducible results</p> <code>None</code> <code>user</code> <code>str | None</code> <p>Unique identifier for the end user</p> <code>None</code> <code>parallel_tool_calls</code> <code>bool | None</code> <p>Whether to allow parallel tool calls</p> <code>None</code> <code>logprobs</code> <code>bool | None</code> <p>Include token-level log probabilities in the response</p> <code>None</code> <code>top_logprobs</code> <code>int | None</code> <p>Number of alternatives to return when logprobs are requested</p> <code>None</code> <code>logit_bias</code> <code>dict[str, float] | None</code> <p>Bias the likelihood of specified tokens during generation</p> <code>None</code> <code>stream_options</code> <code>dict[str, Any] | None</code> <p>Additional options controlling streaming behavior</p> <code>None</code> <code>max_completion_tokens</code> <code>int | None</code> <p>Maximum number of tokens for the completion</p> <code>None</code> <code>reasoning_effort</code> <code>Literal['minimal', 'low', 'medium', 'high', 'auto'] | None</code> <p>Reasoning effort level for models that support it. \"auto\" will map to each provider's default.</p> <code>'auto'</code> <code>**kwargs</code> <code>Any</code> <p>Additional provider-specific arguments that will be passed to the provider's API call.</p> <code>{}</code> <p>Returns:</p> Type Description <code>ChatCompletion | AsyncIterator[ChatCompletionChunk]</code> <p>The completion response from the provider</p> Source code in <code>src/any_llm/any_llm.py</code> <pre><code>async def acompletion(\n    self,\n    model: str,\n    messages: list[dict[str, Any] | ChatCompletionMessage],\n    *,\n    tools: list[dict[str, Any] | Callable[..., Any]] | Any | None = None,\n    tool_choice: str | dict[str, Any] | None = None,\n    temperature: float | None = None,\n    top_p: float | None = None,\n    max_tokens: int | None = None,\n    response_format: dict[str, Any] | type[BaseModel] | None = None,\n    stream: bool | None = None,\n    n: int | None = None,\n    stop: str | list[str] | None = None,\n    presence_penalty: float | None = None,\n    frequency_penalty: float | None = None,\n    seed: int | None = None,\n    user: str | None = None,\n    parallel_tool_calls: bool | None = None,\n    logprobs: bool | None = None,\n    top_logprobs: int | None = None,\n    logit_bias: dict[str, float] | None = None,\n    stream_options: dict[str, Any] | None = None,\n    max_completion_tokens: int | None = None,\n    reasoning_effort: Literal[\"minimal\", \"low\", \"medium\", \"high\", \"auto\"] | None = \"auto\",\n    **kwargs: Any,\n) -&gt; ChatCompletion | AsyncIterator[ChatCompletionChunk]:\n    \"\"\"Create a chat completion asynchronously.\n\n    Args:\n        model: Model identifier for the chosen provider (e.g., model='gpt-4.1-mini' for LLMProvider.OPENAI).\n        messages: List of messages for the conversation\n        tools: List of tools for tool calling. Can be Python callables or OpenAI tool format dicts\n        tool_choice: Controls which tools the model can call\n        temperature: Controls randomness in the response (0.0 to 2.0)\n        top_p: Controls diversity via nucleus sampling (0.0 to 1.0)\n        max_tokens: Maximum number of tokens to generate\n        response_format: Format specification for the response\n        stream: Whether to stream the response\n        n: Number of completions to generate\n        stop: Stop sequences for generation\n        presence_penalty: Penalize new tokens based on presence in text\n        frequency_penalty: Penalize new tokens based on frequency in text\n        seed: Random seed for reproducible results\n        user: Unique identifier for the end user\n        parallel_tool_calls: Whether to allow parallel tool calls\n        logprobs: Include token-level log probabilities in the response\n        top_logprobs: Number of alternatives to return when logprobs are requested\n        logit_bias: Bias the likelihood of specified tokens during generation\n        stream_options: Additional options controlling streaming behavior\n        max_completion_tokens: Maximum number of tokens for the completion\n        reasoning_effort: Reasoning effort level for models that support it. \"auto\" will map to each provider's default.\n        **kwargs: Additional provider-specific arguments that will be passed to the provider's API call.\n\n    Returns:\n        The completion response from the provider\n\n    \"\"\"\n    all_args = locals()\n    all_args.pop(\"self\")\n    all_args[\"model_id\"] = all_args.pop(\"model\")\n    kwargs = all_args.pop(\"kwargs\")\n\n    if tools:\n        all_args[\"tools\"] = prepare_tools(tools, built_in_tools=self.BUILT_IN_TOOLS)\n\n    for i, message in enumerate(messages):\n        if isinstance(message, ChatCompletionMessage):\n            # Dump the message but exclude the extra field that we extend from OpenAI Spec\n            messages[i] = message.model_dump(exclude_none=True, exclude={\"reasoning\"})\n    all_args[\"messages\"] = messages\n\n    return await self._acompletion(CompletionParams(**all_args), **kwargs)\n</code></pre>"},{"location":"api/any_llm/#any_llm.AnyLLM.acreate_batch","title":"<code>acreate_batch(input_file_path, endpoint, completion_window='24h', metadata=None, **kwargs)</code>  <code>async</code>","text":"<p>Create a batch job asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>input_file_path</code> <code>str</code> <p>Path to a local file containing batch requests in JSONL format.</p> required <code>endpoint</code> <code>str</code> <p>The endpoint to be used for all requests (e.g., '/v1/chat/completions')</p> required <code>completion_window</code> <code>str</code> <p>The time frame within which the batch should be processed (default: '24h')</p> <code>'24h'</code> <code>metadata</code> <code>dict[str, str] | None</code> <p>Optional custom metadata for the batch</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional provider-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Batch</code> <p>The created batch object</p> Source code in <code>src/any_llm/any_llm.py</code> <pre><code>@experimental(BATCH_API_EXPERIMENTAL_MESSAGE)\nasync def acreate_batch(\n    self,\n    input_file_path: str,\n    endpoint: str,\n    completion_window: str = \"24h\",\n    metadata: dict[str, str] | None = None,\n    **kwargs: Any,\n) -&gt; Batch:\n    \"\"\"Create a batch job asynchronously.\n\n    Args:\n        input_file_path: Path to a local file containing batch requests in JSONL format.\n        endpoint: The endpoint to be used for all requests (e.g., '/v1/chat/completions')\n        completion_window: The time frame within which the batch should be processed (default: '24h')\n        metadata: Optional custom metadata for the batch\n        **kwargs: Additional provider-specific arguments\n\n    Returns:\n        The created batch object\n\n    \"\"\"\n    return await self._acreate_batch(\n        input_file_path=input_file_path,\n        endpoint=endpoint,\n        completion_window=completion_window,\n        metadata=metadata,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/any_llm/#any_llm.AnyLLM.alist_batches","title":"<code>alist_batches(after=None, limit=None, **kwargs)</code>  <code>async</code>","text":"<p>List batch jobs asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>after</code> <code>str | None</code> <p>A cursor for pagination. Returns batches after this batch ID.</p> <code>None</code> <code>limit</code> <code>int | None</code> <p>Maximum number of batches to return (default: 20)</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional provider-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Sequence[Batch]</code> <p>A list of batch objects</p> Source code in <code>src/any_llm/any_llm.py</code> <pre><code>@experimental(BATCH_API_EXPERIMENTAL_MESSAGE)\nasync def alist_batches(\n    self,\n    after: str | None = None,\n    limit: int | None = None,\n    **kwargs: Any,\n) -&gt; Sequence[Batch]:\n    \"\"\"List batch jobs asynchronously.\n\n    Args:\n        after: A cursor for pagination. Returns batches after this batch ID.\n        limit: Maximum number of batches to return (default: 20)\n        **kwargs: Additional provider-specific arguments\n\n    Returns:\n        A list of batch objects\n\n    \"\"\"\n    return await self._alist_batches(after=after, limit=limit, **kwargs)\n</code></pre>"},{"location":"api/any_llm/#any_llm.AnyLLM.aresponses","title":"<code>aresponses(model, input_data, *, tools=None, tool_choice=None, max_output_tokens=None, temperature=None, top_p=None, stream=None, instructions=None, max_tool_calls=None, parallel_tool_calls=None, reasoning=None, text=None, **kwargs)</code>  <code>async</code>","text":"<p>Create a response using the OpenAI-style Responses API.</p> <p>This follows the OpenAI Responses API shape and returns the aliased <code>any_llm.types.responses.Response</code> type. If <code>stream=True</code>, an iterator of <code>any_llm.types.responses.ResponseStreamEvent</code> items is returned.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Model identifier for the chosen provider (e.g., model='gpt-4.1-mini' for LLMProvider.OPENAI).</p> required <code>input_data</code> <code>str | ResponseInputParam</code> <p>The input payload accepted by provider's Responses API. For OpenAI-compatible providers, this is typically a list mixing text, images, and tool instructions, or a dict per OpenAI spec.</p> required <code>tools</code> <code>list[dict[str, Any] | Callable[..., Any]] | Any | None</code> <p>Optional tools for tool calling (Python callables or OpenAI tool dicts)</p> <code>None</code> <code>tool_choice</code> <code>str | dict[str, Any] | None</code> <p>Controls which tools the model can call</p> <code>None</code> <code>max_output_tokens</code> <code>int | None</code> <p>Maximum number of output tokens to generate</p> <code>None</code> <code>temperature</code> <code>float | None</code> <p>Controls randomness in the response (0.0 to 2.0)</p> <code>None</code> <code>top_p</code> <code>float | None</code> <p>Controls diversity via nucleus sampling (0.0 to 1.0)</p> <code>None</code> <code>stream</code> <code>bool | None</code> <p>Whether to stream response events</p> <code>None</code> <code>instructions</code> <code>str | None</code> <p>A system (or developer) message inserted into the model's context.</p> <code>None</code> <code>max_tool_calls</code> <code>int | None</code> <p>The maximum number of total calls to built-in tools that can be processed in a response. This maximum number applies across all built-in tool calls, not per individual tool. Any further attempts to call a tool by the model will be ignored.</p> <code>None</code> <code>parallel_tool_calls</code> <code>int | None</code> <p>Whether to allow the model to run tool calls in parallel.</p> <code>None</code> <code>reasoning</code> <code>Any | None</code> <p>Configuration options for reasoning models.</p> <code>None</code> <code>text</code> <code>Any | None</code> <p>Configuration options for a text response from the model. Can be plain text or structured JSON data.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional provider-specific arguments that will be passed to the provider's API call.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Response | AsyncIterator[ResponseStreamEvent]</code> <p>Either a <code>Response</code> object (non-streaming) or an iterator of</p> <code>Response | AsyncIterator[ResponseStreamEvent]</code> <p><code>ResponseStreamEvent</code> (streaming).</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the selected provider does not support the Responses API.</p> Source code in <code>src/any_llm/any_llm.py</code> <pre><code>async def aresponses(\n    self,\n    model: str,\n    input_data: str | ResponseInputParam,\n    *,\n    tools: list[dict[str, Any] | Callable[..., Any]] | Any | None = None,\n    tool_choice: str | dict[str, Any] | None = None,\n    max_output_tokens: int | None = None,\n    temperature: float | None = None,\n    top_p: float | None = None,\n    stream: bool | None = None,\n    instructions: str | None = None,\n    max_tool_calls: int | None = None,\n    parallel_tool_calls: int | None = None,\n    reasoning: Any | None = None,\n    text: Any | None = None,\n    **kwargs: Any,\n) -&gt; Response | AsyncIterator[ResponseStreamEvent]:\n    \"\"\"Create a response using the OpenAI-style Responses API.\n\n    This follows the OpenAI Responses API shape and returns the aliased\n    `any_llm.types.responses.Response` type. If `stream=True`, an iterator of\n    `any_llm.types.responses.ResponseStreamEvent` items is returned.\n\n    Args:\n        model: Model identifier for the chosen provider (e.g., model='gpt-4.1-mini' for LLMProvider.OPENAI).\n        input_data: The input payload accepted by provider's Responses API.\n            For OpenAI-compatible providers, this is typically a list mixing\n            text, images, and tool instructions, or a dict per OpenAI spec.\n        tools: Optional tools for tool calling (Python callables or OpenAI tool dicts)\n        tool_choice: Controls which tools the model can call\n        max_output_tokens: Maximum number of output tokens to generate\n        temperature: Controls randomness in the response (0.0 to 2.0)\n        top_p: Controls diversity via nucleus sampling (0.0 to 1.0)\n        stream: Whether to stream response events\n        instructions: A system (or developer) message inserted into the model's context.\n        max_tool_calls: The maximum number of total calls to built-in tools that can be processed in a response. This maximum number applies across all built-in tool calls, not per individual tool. Any further attempts to call a tool by the model will be ignored.\n        parallel_tool_calls: Whether to allow the model to run tool calls in parallel.\n        reasoning: Configuration options for reasoning models.\n        text: Configuration options for a text response from the model. Can be plain text or structured JSON data.\n        **kwargs: Additional provider-specific arguments that will be passed to the provider's API call.\n\n    Returns:\n        Either a `Response` object (non-streaming) or an iterator of\n        `ResponseStreamEvent` (streaming).\n\n    Raises:\n        NotImplementedError: If the selected provider does not support the Responses API.\n\n    \"\"\"\n    all_args = locals()\n    all_args.pop(\"self\")\n    all_args[\"input\"] = all_args.pop(\"input_data\")\n    kwargs = all_args.pop(\"kwargs\")\n\n    if tools:\n        all_args[\"tools\"] = prepare_tools(tools, built_in_tools=self.BUILT_IN_TOOLS)\n\n    return await self._aresponses(ResponsesParams(**all_args, **kwargs))\n</code></pre>"},{"location":"api/any_llm/#any_llm.AnyLLM.aretrieve_batch","title":"<code>aretrieve_batch(batch_id, **kwargs)</code>  <code>async</code>","text":"<p>Retrieve a batch job asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>batch_id</code> <code>str</code> <p>The ID of the batch to retrieve</p> required <code>**kwargs</code> <code>Any</code> <p>Additional provider-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Batch</code> <p>The batch object</p> Source code in <code>src/any_llm/any_llm.py</code> <pre><code>@experimental(BATCH_API_EXPERIMENTAL_MESSAGE)\nasync def aretrieve_batch(self, batch_id: str, **kwargs: Any) -&gt; Batch:\n    \"\"\"Retrieve a batch job asynchronously.\n\n    Args:\n        batch_id: The ID of the batch to retrieve\n        **kwargs: Additional provider-specific arguments\n\n    Returns:\n        The batch object\n\n    \"\"\"\n    return await self._aretrieve_batch(batch_id, **kwargs)\n</code></pre>"},{"location":"api/any_llm/#any_llm.AnyLLM.cancel_batch","title":"<code>cancel_batch(batch_id, **kwargs)</code>","text":"<p>Cancel a batch synchronously.</p> <p>See AnyLLM.acancel_batch</p> Source code in <code>src/any_llm/any_llm.py</code> <pre><code>@experimental(BATCH_API_EXPERIMENTAL_MESSAGE)\ndef cancel_batch(self, batch_id: str, **kwargs: Any) -&gt; Batch:\n    \"\"\"Cancel a batch synchronously.\n\n    See [AnyLLM.acancel_batch][any_llm.any_llm.AnyLLM.acancel_batch]\n    \"\"\"\n    allow_running_loop = kwargs.pop(\"allow_running_loop\", INSIDE_NOTEBOOK)\n    return run_async_in_sync(self.acancel_batch(batch_id, **kwargs), allow_running_loop=allow_running_loop)\n</code></pre>"},{"location":"api/any_llm/#any_llm.AnyLLM.completion","title":"<code>completion(**kwargs)</code>","text":"<p>Create a chat completion synchronously.</p> <p>See AnyLLM.acompletion</p> Source code in <code>src/any_llm/any_llm.py</code> <pre><code>def completion(\n    self,\n    **kwargs: Any,\n) -&gt; ChatCompletion | Iterator[ChatCompletionChunk]:\n    \"\"\"Create a chat completion synchronously.\n\n    See [AnyLLM.acompletion][any_llm.any_llm.AnyLLM.acompletion]\n    \"\"\"\n    allow_running_loop = kwargs.pop(\"allow_running_loop\", INSIDE_NOTEBOOK)\n    response = run_async_in_sync(self.acompletion(**kwargs), allow_running_loop=allow_running_loop)\n    if isinstance(response, ChatCompletion):\n        return response\n\n    return async_iter_to_sync_iter(response)\n</code></pre>"},{"location":"api/any_llm/#any_llm.AnyLLM.create","title":"<code>create(provider, api_key=None, api_base=None, **kwargs)</code>  <code>classmethod</code>","text":"<p>Create a provider instance using the given provider name and config.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str | LLMProvider</code> <p>The provider name (e.g., 'openai', 'anthropic')</p> required <code>api_key</code> <code>str | None</code> <p>API key for the provider</p> <code>None</code> <code>api_base</code> <code>str | None</code> <p>Base URL for the provider API</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional provider-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>AnyLLM</code> <p>Provider instance for the specified provider</p> Source code in <code>src/any_llm/any_llm.py</code> <pre><code>@classmethod\ndef create(\n    cls, provider: str | LLMProvider, api_key: str | None = None, api_base: str | None = None, **kwargs: Any\n) -&gt; AnyLLM:\n    \"\"\"Create a provider instance using the given provider name and config.\n\n    Args:\n        provider: The provider name (e.g., 'openai', 'anthropic')\n        api_key: API key for the provider\n        api_base: Base URL for the provider API\n        **kwargs: Additional provider-specific arguments\n\n    Returns:\n        Provider instance for the specified provider\n\n    \"\"\"\n    return cls._create_provider(provider, api_key=api_key, api_base=api_base, **kwargs)\n</code></pre>"},{"location":"api/any_llm/#any_llm.AnyLLM.create_batch","title":"<code>create_batch(**kwargs)</code>","text":"<p>Create a batch synchronously.</p> <p>See AnyLLM.acreate_batch</p> Source code in <code>src/any_llm/any_llm.py</code> <pre><code>@experimental(BATCH_API_EXPERIMENTAL_MESSAGE)\ndef create_batch(self, **kwargs: Any) -&gt; Batch:\n    \"\"\"Create a batch synchronously.\n\n    See [AnyLLM.acreate_batch][any_llm.any_llm.AnyLLM.acreate_batch]\n    \"\"\"\n    allow_running_loop = kwargs.pop(\"allow_running_loop\", INSIDE_NOTEBOOK)\n    return run_async_in_sync(self.acreate_batch(**kwargs), allow_running_loop=allow_running_loop)\n</code></pre>"},{"location":"api/any_llm/#any_llm.AnyLLM.get_all_provider_metadata","title":"<code>get_all_provider_metadata()</code>  <code>classmethod</code>","text":"<p>Get metadata for all supported providers.</p> <p>Returns:</p> Type Description <code>list[ProviderMetadata]</code> <p>List of dictionaries containing provider metadata</p> Source code in <code>src/any_llm/any_llm.py</code> <pre><code>@classmethod\ndef get_all_provider_metadata(cls) -&gt; list[ProviderMetadata]:\n    \"\"\"Get metadata for all supported providers.\n\n    Returns:\n        List of dictionaries containing provider metadata\n\n    \"\"\"\n    providers: list[ProviderMetadata] = []\n    for provider_key in cls.get_supported_providers():\n        provider_class = cls.get_provider_class(provider_key)\n        metadata = provider_class.get_provider_metadata()\n        providers.append(metadata)\n\n    # Sort providers by name\n    providers.sort(key=lambda x: x.name)\n    return providers\n</code></pre>"},{"location":"api/any_llm/#any_llm.AnyLLM.get_provider_class","title":"<code>get_provider_class(provider_key)</code>  <code>classmethod</code>","text":"<p>Get the provider class without instantiating it.</p> <p>Parameters:</p> Name Type Description Default <code>provider_key</code> <code>str | LLMProvider</code> <p>The provider key (e.g., 'anthropic', 'openai')</p> required <p>Returns:</p> Type Description <code>type[AnyLLM]</code> <p>The provider class</p> Source code in <code>src/any_llm/any_llm.py</code> <pre><code>@classmethod\ndef get_provider_class(cls, provider_key: str | LLMProvider) -&gt; type[AnyLLM]:\n    \"\"\"Get the provider class without instantiating it.\n\n    Args:\n        provider_key: The provider key (e.g., 'anthropic', 'openai')\n\n    Returns:\n        The provider class\n\n    \"\"\"\n    provider_key = LLMProvider.from_string(provider_key).value\n\n    provider_class_name = f\"{provider_key.capitalize()}Provider\"\n    provider_module_name = f\"{provider_key}\"\n\n    module_path = f\"any_llm.providers.{provider_module_name}\"\n\n    try:\n        module = importlib.import_module(module_path)\n    except ImportError as e:\n        msg = f\"Could not import module {module_path}: {e!s}. Please ensure the provider is supported by doing AnyLLM.get_supported_providers()\"\n        raise ImportError(msg) from e\n\n    provider_class: type[AnyLLM] = getattr(module, provider_class_name)\n    return provider_class\n</code></pre>"},{"location":"api/any_llm/#any_llm.AnyLLM.get_provider_enum","title":"<code>get_provider_enum(provider_key)</code>  <code>classmethod</code>","text":"<p>Convert a string provider key to a ProviderName enum.</p> Source code in <code>src/any_llm/any_llm.py</code> <pre><code>@classmethod\ndef get_provider_enum(cls, provider_key: str) -&gt; LLMProvider:\n    \"\"\"Convert a string provider key to a ProviderName enum.\"\"\"\n    try:\n        return LLMProvider(provider_key)\n    except ValueError as e:\n        supported = [provider.value for provider in LLMProvider]\n        raise UnsupportedProviderError(provider_key, supported) from e\n</code></pre>"},{"location":"api/any_llm/#any_llm.AnyLLM.get_provider_metadata","title":"<code>get_provider_metadata()</code>  <code>classmethod</code>","text":"<p>Get provider metadata without requiring instantiation.</p> <p>Returns:</p> Type Description <code>ProviderMetadata</code> <p>Dictionary containing provider metadata including name, environment variable,</p> <code>ProviderMetadata</code> <p>documentation URL, and class name.</p> Source code in <code>src/any_llm/any_llm.py</code> <pre><code>@classmethod\ndef get_provider_metadata(cls) -&gt; ProviderMetadata:\n    \"\"\"Get provider metadata without requiring instantiation.\n\n    Returns:\n        Dictionary containing provider metadata including name, environment variable,\n        documentation URL, and class name.\n\n    \"\"\"\n    return ProviderMetadata(\n        name=cls.PROVIDER_NAME,\n        env_key=cls.ENV_API_KEY_NAME,\n        doc_url=cls.PROVIDER_DOCUMENTATION_URL,\n        streaming=cls.SUPPORTS_COMPLETION_STREAMING,\n        reasoning=cls.SUPPORTS_COMPLETION_REASONING,\n        completion=cls.SUPPORTS_COMPLETION,\n        image=cls.SUPPORTS_COMPLETION_IMAGE,\n        pdf=cls.SUPPORTS_COMPLETION_PDF,\n        embedding=cls.SUPPORTS_EMBEDDING,\n        responses=cls.SUPPORTS_RESPONSES,\n        list_models=cls.SUPPORTS_LIST_MODELS,\n        batch_completion=cls.SUPPORTS_BATCH,\n        class_name=cls.__name__,\n    )\n</code></pre>"},{"location":"api/any_llm/#any_llm.AnyLLM.get_supported_providers","title":"<code>get_supported_providers()</code>  <code>classmethod</code>","text":"<p>Get a list of supported provider keys.</p> Source code in <code>src/any_llm/any_llm.py</code> <pre><code>@classmethod\ndef get_supported_providers(cls) -&gt; list[str]:\n    \"\"\"Get a list of supported provider keys.\"\"\"\n    return [provider.value for provider in LLMProvider]\n</code></pre>"},{"location":"api/any_llm/#any_llm.AnyLLM.list_batches","title":"<code>list_batches(after=None, limit=None, **kwargs)</code>","text":"<p>List batches synchronously.</p> <p>See AnyLLM.alist_batches</p> Source code in <code>src/any_llm/any_llm.py</code> <pre><code>@experimental(BATCH_API_EXPERIMENTAL_MESSAGE)\ndef list_batches(\n    self,\n    after: str | None = None,\n    limit: int | None = None,\n    **kwargs: Any,\n) -&gt; Sequence[Batch]:\n    \"\"\"List batches synchronously.\n\n    See [AnyLLM.alist_batches][any_llm.any_llm.AnyLLM.alist_batches]\n    \"\"\"\n    allow_running_loop = kwargs.pop(\"allow_running_loop\", INSIDE_NOTEBOOK)\n    return run_async_in_sync(\n        self.alist_batches(after=after, limit=limit, **kwargs), allow_running_loop=allow_running_loop\n    )\n</code></pre>"},{"location":"api/any_llm/#any_llm.AnyLLM.responses","title":"<code>responses(**kwargs)</code>","text":"<p>Create a response synchronously.</p> <p>See AnyLLM.aresponses</p> Source code in <code>src/any_llm/any_llm.py</code> <pre><code>def responses(self, **kwargs: Any) -&gt; Response | Iterator[ResponseStreamEvent]:\n    \"\"\"Create a response synchronously.\n\n    See [AnyLLM.aresponses][any_llm.any_llm.AnyLLM.aresponses]\n    \"\"\"\n    allow_running_loop = kwargs.pop(\"allow_running_loop\", INSIDE_NOTEBOOK)\n    response = run_async_in_sync(self.aresponses(**kwargs), allow_running_loop=allow_running_loop)\n    if isinstance(response, Response):\n        return response\n    return async_iter_to_sync_iter(response)\n</code></pre>"},{"location":"api/any_llm/#any_llm.AnyLLM.retrieve_batch","title":"<code>retrieve_batch(batch_id, **kwargs)</code>","text":"<p>Retrieve a batch synchronously.</p> <p>See AnyLLM.aretrieve_batch</p> Source code in <code>src/any_llm/any_llm.py</code> <pre><code>@experimental(BATCH_API_EXPERIMENTAL_MESSAGE)\ndef retrieve_batch(self, batch_id: str, **kwargs: Any) -&gt; Batch:\n    \"\"\"Retrieve a batch synchronously.\n\n    See [AnyLLM.aretrieve_batch][any_llm.any_llm.AnyLLM.aretrieve_batch]\n    \"\"\"\n    allow_running_loop = kwargs.pop(\"allow_running_loop\", INSIDE_NOTEBOOK)\n    return run_async_in_sync(self.aretrieve_batch(batch_id, **kwargs), allow_running_loop=allow_running_loop)\n</code></pre>"},{"location":"api/any_llm/#any_llm.AnyLLM.split_model_provider","title":"<code>split_model_provider(model)</code>  <code>classmethod</code>","text":"<p>Extract the provider key from the model identifier.</p> <p>Supports both new format 'provider:model' (e.g., 'mistral:mistral-small') and legacy format 'provider/model' (e.g., 'mistral/mistral-small').</p> <p>The legacy format will be deprecated in version 1.0.</p> Source code in <code>src/any_llm/any_llm.py</code> <pre><code>@classmethod\ndef split_model_provider(cls, model: str) -&gt; tuple[LLMProvider, str]:\n    \"\"\"Extract the provider key from the model identifier.\n\n    Supports both new format 'provider:model' (e.g., 'mistral:mistral-small')\n    and legacy format 'provider/model' (e.g., 'mistral/mistral-small').\n\n    The legacy format will be deprecated in version 1.0.\n    \"\"\"\n    colon_index = model.find(\":\")\n    slash_index = model.find(\"/\")\n\n    # Determine which delimiter comes first\n    if colon_index != -1 and (slash_index == -1 or colon_index &lt; slash_index):\n        # The colon came first, so it's using the new syntax.\n        provider, model_name = model.split(\":\", 1)\n    elif slash_index != -1:\n        # Slash comes first, so it's the legacy syntax\n        warnings.warn(\n            f\"Model format 'provider/model' is deprecated and will be removed in version 1.0. \"\n            f\"Please use 'provider:model' format instead. Got: '{model}'\",\n            DeprecationWarning,\n            stacklevel=3,\n        )\n        provider, model_name = model.split(\"/\", 1)\n    else:\n        msg = f\"Invalid model format. Expected 'provider:model' or 'provider/model', got '{model}'\"\n        raise ValueError(msg)\n\n    if not provider or not model_name:\n        msg = f\"Invalid model format. Expected 'provider:model' or 'provider/model', got '{model}'\"\n        raise ValueError(msg)\n    return cls.get_provider_enum(provider), model_name\n</code></pre>"},{"location":"api/batch/","title":"Batch","text":"<p>Experimental API</p> <p>The Batch API is experimental and subject to breaking changes in future versions. Use with caution in production environments.</p> <p>The Batch API allows you to process multiple requests asynchronously at a lower cost.</p>"},{"location":"api/batch/#file-path-interface","title":"File Path Interface","text":"<p>The <code>any-llm</code> batch API requires you to pass a path to a local JSONL file containing your batch requests. The provider implementation automatically handles uploading and file management as needed.</p> <p>Different providers handle batch processing differently:</p> <ul> <li>OpenAI: Requires uploading a file first, then creating a batch with the file ID</li> <li>Anthropic (future): Expects file content passed directly in the request</li> <li>Other providers: May have their own unique requirements</li> </ul> <p>By accepting a local file path, <code>any-llm</code> abstracts these provider differences and handles the implementation details automatically.</p>"},{"location":"api/batch/#any_llm.api.create_batch","title":"<code>any_llm.api.create_batch(provider, input_file_path, endpoint, *, completion_window='24h', metadata=None, api_key=None, api_base=None, client_args=None, **kwargs)</code>","text":"<p>Create a batch job.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str | LLMProvider</code> <p>Provider name to use for the request (e.g., 'openai', 'mistral')</p> required <code>input_file_path</code> <code>str</code> <p>Path to a local file containing batch requests in JSONL format.</p> required <code>endpoint</code> <code>str</code> <p>The endpoint to be used for all requests (e.g., '/v1/chat/completions')</p> required <code>completion_window</code> <code>str</code> <p>The time frame within which the batch should be processed (default: '24h')</p> <code>'24h'</code> <code>metadata</code> <code>dict[str, str] | None</code> <p>Optional custom metadata for the batch</p> <code>None</code> <code>api_key</code> <code>str | None</code> <p>API key for the provider</p> <code>None</code> <code>api_base</code> <code>str | None</code> <p>Base URL for the provider API</p> <code>None</code> <code>client_args</code> <code>dict[str, Any] | None</code> <p>Additional provider-specific arguments for client instantiation</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional provider-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Batch</code> <p>The created batch object</p> Source code in <code>src/any_llm/api.py</code> <pre><code>@experimental(BATCH_API_EXPERIMENTAL_MESSAGE)\ndef create_batch(\n    provider: str | LLMProvider,\n    input_file_path: str,\n    endpoint: str,\n    *,\n    completion_window: str = \"24h\",\n    metadata: dict[str, str] | None = None,\n    api_key: str | None = None,\n    api_base: str | None = None,\n    client_args: dict[str, Any] | None = None,\n    **kwargs: Any,\n) -&gt; Batch:\n    \"\"\"Create a batch job.\n\n    Args:\n        provider: Provider name to use for the request (e.g., 'openai', 'mistral')\n        input_file_path: Path to a local file containing batch requests in JSONL format.\n        endpoint: The endpoint to be used for all requests (e.g., '/v1/chat/completions')\n        completion_window: The time frame within which the batch should be processed (default: '24h')\n        metadata: Optional custom metadata for the batch\n        api_key: API key for the provider\n        api_base: Base URL for the provider API\n        client_args: Additional provider-specific arguments for client instantiation\n        **kwargs: Additional provider-specific arguments\n\n    Returns:\n        The created batch object\n\n    \"\"\"\n    llm = AnyLLM.create(LLMProvider.from_string(provider), api_key=api_key, api_base=api_base, **client_args or {})\n    return llm.create_batch(\n        input_file_path=input_file_path,\n        endpoint=endpoint,\n        completion_window=completion_window,\n        metadata=metadata,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/batch/#any_llm.api.acreate_batch","title":"<code>any_llm.api.acreate_batch(provider, input_file_path, endpoint, *, completion_window='24h', metadata=None, api_key=None, api_base=None, client_args=None, **kwargs)</code>  <code>async</code>","text":"<p>Create a batch job asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str | LLMProvider</code> <p>Provider name to use for the request (e.g., 'openai', 'mistral')</p> required <code>input_file_path</code> <code>str</code> <p>Path to a local file containing batch requests in JSONL format.</p> required <code>endpoint</code> <code>str</code> <p>The endpoint to be used for all requests (e.g., '/v1/chat/completions')</p> required <code>completion_window</code> <code>str</code> <p>The time frame within which the batch should be processed (default: '24h')</p> <code>'24h'</code> <code>metadata</code> <code>dict[str, str] | None</code> <p>Optional custom metadata for the batch</p> <code>None</code> <code>api_key</code> <code>str | None</code> <p>API key for the provider</p> <code>None</code> <code>api_base</code> <code>str | None</code> <p>Base URL for the provider API</p> <code>None</code> <code>client_args</code> <code>dict[str, Any] | None</code> <p>Additional provider-specific arguments for client instantiation</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional provider-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Batch</code> <p>The created batch object</p> Source code in <code>src/any_llm/api.py</code> <pre><code>@experimental(BATCH_API_EXPERIMENTAL_MESSAGE)\nasync def acreate_batch(\n    provider: str | LLMProvider,\n    input_file_path: str,\n    endpoint: str,\n    *,\n    completion_window: str = \"24h\",\n    metadata: dict[str, str] | None = None,\n    api_key: str | None = None,\n    api_base: str | None = None,\n    client_args: dict[str, Any] | None = None,\n    **kwargs: Any,\n) -&gt; Batch:\n    \"\"\"Create a batch job asynchronously.\n\n    Args:\n        provider: Provider name to use for the request (e.g., 'openai', 'mistral')\n        input_file_path: Path to a local file containing batch requests in JSONL format.\n        endpoint: The endpoint to be used for all requests (e.g., '/v1/chat/completions')\n        completion_window: The time frame within which the batch should be processed (default: '24h')\n        metadata: Optional custom metadata for the batch\n        api_key: API key for the provider\n        api_base: Base URL for the provider API\n        client_args: Additional provider-specific arguments for client instantiation\n        **kwargs: Additional provider-specific arguments\n\n    Returns:\n        The created batch object\n\n    \"\"\"\n    llm = AnyLLM.create(LLMProvider.from_string(provider), api_key=api_key, api_base=api_base, **client_args or {})\n    return await llm.acreate_batch(\n        input_file_path=input_file_path,\n        endpoint=endpoint,\n        completion_window=completion_window,\n        metadata=metadata,\n        **kwargs,\n    )\n</code></pre>"},{"location":"api/batch/#any_llm.api.retrieve_batch","title":"<code>any_llm.api.retrieve_batch(provider, batch_id, *, api_key=None, api_base=None, client_args=None, **kwargs)</code>","text":"<p>Retrieve a batch job.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str | LLMProvider</code> <p>Provider name to use for the request (e.g., 'openai', 'mistral')</p> required <code>batch_id</code> <code>str</code> <p>The ID of the batch to retrieve</p> required <code>api_key</code> <code>str | None</code> <p>API key for the provider</p> <code>None</code> <code>api_base</code> <code>str | None</code> <p>Base URL for the provider API</p> <code>None</code> <code>client_args</code> <code>dict[str, Any] | None</code> <p>Additional provider-specific arguments for client instantiation</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional provider-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Batch</code> <p>The batch object</p> Source code in <code>src/any_llm/api.py</code> <pre><code>@experimental(BATCH_API_EXPERIMENTAL_MESSAGE)\ndef retrieve_batch(\n    provider: str | LLMProvider,\n    batch_id: str,\n    *,\n    api_key: str | None = None,\n    api_base: str | None = None,\n    client_args: dict[str, Any] | None = None,\n    **kwargs: Any,\n) -&gt; Batch:\n    \"\"\"Retrieve a batch job.\n\n    Args:\n        provider: Provider name to use for the request (e.g., 'openai', 'mistral')\n        batch_id: The ID of the batch to retrieve\n        api_key: API key for the provider\n        api_base: Base URL for the provider API\n        client_args: Additional provider-specific arguments for client instantiation\n        **kwargs: Additional provider-specific arguments\n\n    Returns:\n        The batch object\n\n    \"\"\"\n    llm = AnyLLM.create(LLMProvider.from_string(provider), api_key=api_key, api_base=api_base, **client_args or {})\n    return llm.retrieve_batch(batch_id, **kwargs)\n</code></pre>"},{"location":"api/batch/#any_llm.api.aretrieve_batch","title":"<code>any_llm.api.aretrieve_batch(provider, batch_id, *, api_key=None, api_base=None, client_args=None, **kwargs)</code>  <code>async</code>","text":"<p>Retrieve a batch job asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str | LLMProvider</code> <p>Provider name to use for the request (e.g., 'openai', 'mistral')</p> required <code>batch_id</code> <code>str</code> <p>The ID of the batch to retrieve</p> required <code>api_key</code> <code>str | None</code> <p>API key for the provider</p> <code>None</code> <code>api_base</code> <code>str | None</code> <p>Base URL for the provider API</p> <code>None</code> <code>client_args</code> <code>dict[str, Any] | None</code> <p>Additional provider-specific arguments for client instantiation</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional provider-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Batch</code> <p>The batch object</p> Source code in <code>src/any_llm/api.py</code> <pre><code>@experimental(BATCH_API_EXPERIMENTAL_MESSAGE)\nasync def aretrieve_batch(\n    provider: str | LLMProvider,\n    batch_id: str,\n    *,\n    api_key: str | None = None,\n    api_base: str | None = None,\n    client_args: dict[str, Any] | None = None,\n    **kwargs: Any,\n) -&gt; Batch:\n    \"\"\"Retrieve a batch job asynchronously.\n\n    Args:\n        provider: Provider name to use for the request (e.g., 'openai', 'mistral')\n        batch_id: The ID of the batch to retrieve\n        api_key: API key for the provider\n        api_base: Base URL for the provider API\n        client_args: Additional provider-specific arguments for client instantiation\n        **kwargs: Additional provider-specific arguments\n\n    Returns:\n        The batch object\n\n    \"\"\"\n    llm = AnyLLM.create(LLMProvider.from_string(provider), api_key=api_key, api_base=api_base, **client_args or {})\n    return await llm.aretrieve_batch(batch_id, **kwargs)\n</code></pre>"},{"location":"api/batch/#any_llm.api.cancel_batch","title":"<code>any_llm.api.cancel_batch(provider, batch_id, *, api_key=None, api_base=None, client_args=None, **kwargs)</code>","text":"<p>Cancel a batch job.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str | LLMProvider</code> <p>Provider name to use for the request (e.g., 'openai', 'mistral')</p> required <code>batch_id</code> <code>str</code> <p>The ID of the batch to cancel</p> required <code>api_key</code> <code>str | None</code> <p>API key for the provider</p> <code>None</code> <code>api_base</code> <code>str | None</code> <p>Base URL for the provider API</p> <code>None</code> <code>client_args</code> <code>dict[str, Any] | None</code> <p>Additional provider-specific arguments for client instantiation</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional provider-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Batch</code> <p>The cancelled batch object</p> Source code in <code>src/any_llm/api.py</code> <pre><code>@experimental(BATCH_API_EXPERIMENTAL_MESSAGE)\ndef cancel_batch(\n    provider: str | LLMProvider,\n    batch_id: str,\n    *,\n    api_key: str | None = None,\n    api_base: str | None = None,\n    client_args: dict[str, Any] | None = None,\n    **kwargs: Any,\n) -&gt; Batch:\n    \"\"\"Cancel a batch job.\n\n    Args:\n        provider: Provider name to use for the request (e.g., 'openai', 'mistral')\n        batch_id: The ID of the batch to cancel\n        api_key: API key for the provider\n        api_base: Base URL for the provider API\n        client_args: Additional provider-specific arguments for client instantiation\n        **kwargs: Additional provider-specific arguments\n\n    Returns:\n        The cancelled batch object\n\n    \"\"\"\n    llm = AnyLLM.create(LLMProvider.from_string(provider), api_key=api_key, api_base=api_base, **client_args or {})\n    return llm.cancel_batch(batch_id, **kwargs)\n</code></pre>"},{"location":"api/batch/#any_llm.api.acancel_batch","title":"<code>any_llm.api.acancel_batch(provider, batch_id, *, api_key=None, api_base=None, client_args=None, **kwargs)</code>  <code>async</code>","text":"<p>Cancel a batch job asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str | LLMProvider</code> <p>Provider name to use for the request (e.g., 'openai', 'mistral')</p> required <code>batch_id</code> <code>str</code> <p>The ID of the batch to cancel</p> required <code>api_key</code> <code>str | None</code> <p>API key for the provider</p> <code>None</code> <code>api_base</code> <code>str | None</code> <p>Base URL for the provider API</p> <code>None</code> <code>client_args</code> <code>dict[str, Any] | None</code> <p>Additional provider-specific arguments for client instantiation</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional provider-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Batch</code> <p>The cancelled batch object</p> Source code in <code>src/any_llm/api.py</code> <pre><code>@experimental(BATCH_API_EXPERIMENTAL_MESSAGE)\nasync def acancel_batch(\n    provider: str | LLMProvider,\n    batch_id: str,\n    *,\n    api_key: str | None = None,\n    api_base: str | None = None,\n    client_args: dict[str, Any] | None = None,\n    **kwargs: Any,\n) -&gt; Batch:\n    \"\"\"Cancel a batch job asynchronously.\n\n    Args:\n        provider: Provider name to use for the request (e.g., 'openai', 'mistral')\n        batch_id: The ID of the batch to cancel\n        api_key: API key for the provider\n        api_base: Base URL for the provider API\n        client_args: Additional provider-specific arguments for client instantiation\n        **kwargs: Additional provider-specific arguments\n\n    Returns:\n        The cancelled batch object\n\n    \"\"\"\n    llm = AnyLLM.create(LLMProvider.from_string(provider), api_key=api_key, api_base=api_base, **client_args or {})\n    return await llm.acancel_batch(batch_id, **kwargs)\n</code></pre>"},{"location":"api/batch/#any_llm.api.list_batches","title":"<code>any_llm.api.list_batches(provider, *, after=None, limit=None, api_key=None, api_base=None, client_args=None, **kwargs)</code>","text":"<p>List batch jobs.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str | LLMProvider</code> <p>Provider name to use for the request (e.g., 'openai', 'mistral')</p> required <code>after</code> <code>str | None</code> <p>A cursor for pagination. Returns batches after this batch ID.</p> <code>None</code> <code>limit</code> <code>int | None</code> <p>Maximum number of batches to return (default: 20)</p> <code>None</code> <code>api_key</code> <code>str | None</code> <p>API key for the provider</p> <code>None</code> <code>api_base</code> <code>str | None</code> <p>Base URL for the provider API</p> <code>None</code> <code>client_args</code> <code>dict[str, Any] | None</code> <p>Additional provider-specific arguments for client instantiation</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional provider-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Sequence[Batch]</code> <p>A list of batch objects</p> Source code in <code>src/any_llm/api.py</code> <pre><code>@experimental(BATCH_API_EXPERIMENTAL_MESSAGE)\ndef list_batches(\n    provider: str | LLMProvider,\n    *,\n    after: str | None = None,\n    limit: int | None = None,\n    api_key: str | None = None,\n    api_base: str | None = None,\n    client_args: dict[str, Any] | None = None,\n    **kwargs: Any,\n) -&gt; Sequence[Batch]:\n    \"\"\"List batch jobs.\n\n    Args:\n        provider: Provider name to use for the request (e.g., 'openai', 'mistral')\n        after: A cursor for pagination. Returns batches after this batch ID.\n        limit: Maximum number of batches to return (default: 20)\n        api_key: API key for the provider\n        api_base: Base URL for the provider API\n        client_args: Additional provider-specific arguments for client instantiation\n        **kwargs: Additional provider-specific arguments\n\n    Returns:\n        A list of batch objects\n\n    \"\"\"\n    llm = AnyLLM.create(LLMProvider.from_string(provider), api_key=api_key, api_base=api_base, **client_args or {})\n    return llm.list_batches(after=after, limit=limit, **kwargs)\n</code></pre>"},{"location":"api/batch/#any_llm.api.alist_batches","title":"<code>any_llm.api.alist_batches(provider, *, after=None, limit=None, api_key=None, api_base=None, client_args=None, **kwargs)</code>  <code>async</code>","text":"<p>List batch jobs asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>provider</code> <code>str | LLMProvider</code> <p>Provider name to use for the request (e.g., 'openai', 'mistral')</p> required <code>after</code> <code>str | None</code> <p>A cursor for pagination. Returns batches after this batch ID.</p> <code>None</code> <code>limit</code> <code>int | None</code> <p>Maximum number of batches to return (default: 20)</p> <code>None</code> <code>api_key</code> <code>str | None</code> <p>API key for the provider</p> <code>None</code> <code>api_base</code> <code>str | None</code> <p>Base URL for the provider API</p> <code>None</code> <code>client_args</code> <code>dict[str, Any] | None</code> <p>Additional provider-specific arguments for client instantiation</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional provider-specific arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>Sequence[Batch]</code> <p>A list of batch objects</p> Source code in <code>src/any_llm/api.py</code> <pre><code>@experimental(BATCH_API_EXPERIMENTAL_MESSAGE)\nasync def alist_batches(\n    provider: str | LLMProvider,\n    *,\n    after: str | None = None,\n    limit: int | None = None,\n    api_key: str | None = None,\n    api_base: str | None = None,\n    client_args: dict[str, Any] | None = None,\n    **kwargs: Any,\n) -&gt; Sequence[Batch]:\n    \"\"\"List batch jobs asynchronously.\n\n    Args:\n        provider: Provider name to use for the request (e.g., 'openai', 'mistral')\n        after: A cursor for pagination. Returns batches after this batch ID.\n        limit: Maximum number of batches to return (default: 20)\n        api_key: API key for the provider\n        api_base: Base URL for the provider API\n        client_args: Additional provider-specific arguments for client instantiation\n        **kwargs: Additional provider-specific arguments\n\n    Returns:\n        A list of batch objects\n\n    \"\"\"\n    llm = AnyLLM.create(LLMProvider.from_string(provider), api_key=api_key, api_base=api_base, **client_args or {})\n    return await llm.alist_batches(after=after, limit=limit, **kwargs)\n</code></pre>"},{"location":"api/completion/","title":"Completion","text":""},{"location":"api/completion/#completion","title":"Completion","text":""},{"location":"api/completion/#any_llm.api.completion","title":"<code>any_llm.api.completion(model, messages, *, provider=None, tools=None, tool_choice=None, temperature=None, top_p=None, max_tokens=None, response_format=None, stream=None, n=None, stop=None, presence_penalty=None, frequency_penalty=None, seed=None, api_key=None, api_base=None, user=None, parallel_tool_calls=None, logprobs=None, top_logprobs=None, logit_bias=None, stream_options=None, max_completion_tokens=None, reasoning_effort='auto', client_args=None, **kwargs)</code>","text":"<p>Create a chat completion.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Model identifier. Recommended: Use with separate <code>provider</code> parameter (e.g., model='gpt-4', provider='openai'). Alternative: Combined format 'provider:model' (e.g., 'openai:gpt-4'). Legacy format 'provider/model' is also supported but deprecated.</p> required <code>provider</code> <code>str | LLMProvider | None</code> <p>Recommended: Provider name to use for the request (e.g., 'openai', 'mistral'). When provided, the model parameter should contain only the model name.</p> <code>None</code> <code>messages</code> <code>list[dict[str, Any] | ChatCompletionMessage]</code> <p>List of messages for the conversation</p> required <code>tools</code> <code>list[dict[str, Any] | Callable[..., Any]] | None</code> <p>List of tools for tool calling. Can be Python callables or OpenAI tool format dicts</p> <code>None</code> <code>tool_choice</code> <code>str | dict[str, Any] | None</code> <p>Controls which tools the model can call</p> <code>None</code> <code>temperature</code> <code>float | None</code> <p>Controls randomness in the response (0.0 to 2.0)</p> <code>None</code> <code>top_p</code> <code>float | None</code> <p>Controls diversity via nucleus sampling (0.0 to 1.0)</p> <code>None</code> <code>max_tokens</code> <code>int | None</code> <p>Maximum number of tokens to generate</p> <code>None</code> <code>response_format</code> <code>dict[str, Any] | type[BaseModel] | None</code> <p>Format specification for the response</p> <code>None</code> <code>stream</code> <code>bool | None</code> <p>Whether to stream the response</p> <code>None</code> <code>n</code> <code>int | None</code> <p>Number of completions to generate</p> <code>None</code> <code>stop</code> <code>str | list[str] | None</code> <p>Stop sequences for generation</p> <code>None</code> <code>presence_penalty</code> <code>float | None</code> <p>Penalize new tokens based on presence in text</p> <code>None</code> <code>frequency_penalty</code> <code>float | None</code> <p>Penalize new tokens based on frequency in text</p> <code>None</code> <code>seed</code> <code>int | None</code> <p>Random seed for reproducible results</p> <code>None</code> <code>api_key</code> <code>str | None</code> <p>API key for the provider</p> <code>None</code> <code>api_base</code> <code>str | None</code> <p>Base URL for the provider API</p> <code>None</code> <code>user</code> <code>str | None</code> <p>Unique identifier for the end user</p> <code>None</code> <code>parallel_tool_calls</code> <code>bool | None</code> <p>Whether to allow parallel tool calls</p> <code>None</code> <code>logprobs</code> <code>bool | None</code> <p>Include token-level log probabilities in the response</p> <code>None</code> <code>top_logprobs</code> <code>int | None</code> <p>Number of alternatives to return when logprobs are requested</p> <code>None</code> <code>logit_bias</code> <code>dict[str, float] | None</code> <p>Bias the likelihood of specified tokens during generation</p> <code>None</code> <code>stream_options</code> <code>dict[str, Any] | None</code> <p>Additional options controlling streaming behavior</p> <code>None</code> <code>max_completion_tokens</code> <code>int | None</code> <p>Maximum number of tokens for the completion</p> <code>None</code> <code>reasoning_effort</code> <code>Literal['minimal', 'low', 'medium', 'high', 'auto'] | None</code> <p>Reasoning effort level for models that support it. \"auto\" will map to each provider's default.</p> <code>'auto'</code> <code>client_args</code> <code>dict[str, Any] | None</code> <p>Additional provider-specific arguments that will be passed to the provider's client instantiation.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional provider-specific arguments that will be passed to the provider's API call.</p> <code>{}</code> <p>Returns:</p> Type Description <code>ChatCompletion | Iterator[ChatCompletionChunk]</code> <p>The completion response from the provider</p> Source code in <code>src/any_llm/api.py</code> <pre><code>def completion(\n    model: str,\n    messages: list[dict[str, Any] | ChatCompletionMessage],\n    *,\n    provider: str | LLMProvider | None = None,\n    tools: list[dict[str, Any] | Callable[..., Any]] | None = None,\n    tool_choice: str | dict[str, Any] | None = None,\n    temperature: float | None = None,\n    top_p: float | None = None,\n    max_tokens: int | None = None,\n    response_format: dict[str, Any] | type[BaseModel] | None = None,\n    stream: bool | None = None,\n    n: int | None = None,\n    stop: str | list[str] | None = None,\n    presence_penalty: float | None = None,\n    frequency_penalty: float | None = None,\n    seed: int | None = None,\n    api_key: str | None = None,\n    api_base: str | None = None,\n    user: str | None = None,\n    parallel_tool_calls: bool | None = None,\n    logprobs: bool | None = None,\n    top_logprobs: int | None = None,\n    logit_bias: dict[str, float] | None = None,\n    stream_options: dict[str, Any] | None = None,\n    max_completion_tokens: int | None = None,\n    reasoning_effort: Literal[\"minimal\", \"low\", \"medium\", \"high\", \"auto\"] | None = \"auto\",\n    client_args: dict[str, Any] | None = None,\n    **kwargs: Any,\n) -&gt; ChatCompletion | Iterator[ChatCompletionChunk]:\n    \"\"\"Create a chat completion.\n\n    Args:\n        model: Model identifier. **Recommended**: Use with separate `provider` parameter (e.g., model='gpt-4', provider='openai').\n            **Alternative**: Combined format 'provider:model' (e.g., 'openai:gpt-4').\n            Legacy format 'provider/model' is also supported but deprecated.\n        provider: **Recommended**: Provider name to use for the request (e.g., 'openai', 'mistral').\n            When provided, the model parameter should contain only the model name.\n        messages: List of messages for the conversation\n        tools: List of tools for tool calling. Can be Python callables or OpenAI tool format dicts\n        tool_choice: Controls which tools the model can call\n        temperature: Controls randomness in the response (0.0 to 2.0)\n        top_p: Controls diversity via nucleus sampling (0.0 to 1.0)\n        max_tokens: Maximum number of tokens to generate\n        response_format: Format specification for the response\n        stream: Whether to stream the response\n        n: Number of completions to generate\n        stop: Stop sequences for generation\n        presence_penalty: Penalize new tokens based on presence in text\n        frequency_penalty: Penalize new tokens based on frequency in text\n        seed: Random seed for reproducible results\n        api_key: API key for the provider\n        api_base: Base URL for the provider API\n        user: Unique identifier for the end user\n        parallel_tool_calls: Whether to allow parallel tool calls\n        logprobs: Include token-level log probabilities in the response\n        top_logprobs: Number of alternatives to return when logprobs are requested\n        logit_bias: Bias the likelihood of specified tokens during generation\n        stream_options: Additional options controlling streaming behavior\n        max_completion_tokens: Maximum number of tokens for the completion\n        reasoning_effort: Reasoning effort level for models that support it. \"auto\" will map to each provider's default.\n        client_args: Additional provider-specific arguments that will be passed to the provider's client instantiation.\n        **kwargs: Additional provider-specific arguments that will be passed to the provider's API call.\n\n    Returns:\n        The completion response from the provider\n\n    \"\"\"\n    all_args = locals()\n    all_args.pop(\"provider\")\n    kwargs = all_args.pop(\"kwargs\")\n\n    model = all_args.pop(\"model\")\n    if provider is None:\n        provider_key, model_id = AnyLLM.split_model_provider(model)\n    else:\n        provider_key = LLMProvider.from_string(provider)\n        model_id = model\n    all_args[\"model\"] = model_id\n\n    llm = AnyLLM.create(\n        provider_key,\n        api_key=all_args.pop(\"api_key\"),\n        api_base=all_args.pop(\"api_base\"),\n        **all_args.pop(\"client_args\") or {},\n    )\n    return llm.completion(**all_args, **kwargs)\n</code></pre>"},{"location":"api/completion/#any_llm.api.acompletion","title":"<code>any_llm.api.acompletion(model, messages, *, provider=None, tools=None, tool_choice=None, temperature=None, top_p=None, max_tokens=None, response_format=None, stream=None, n=None, stop=None, presence_penalty=None, frequency_penalty=None, seed=None, api_key=None, api_base=None, user=None, parallel_tool_calls=None, logprobs=None, top_logprobs=None, logit_bias=None, stream_options=None, max_completion_tokens=None, reasoning_effort='auto', client_args=None, **kwargs)</code>  <code>async</code>","text":"<p>Create a chat completion asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Model identifier. Recommended: Use with separate <code>provider</code> parameter (e.g., model='gpt-4', provider='openai'). Alternative: Combined format 'provider:model' (e.g., 'openai:gpt-4'). Legacy format 'provider/model' is also supported but deprecated.</p> required <code>provider</code> <code>str | LLMProvider | None</code> <p>Recommended: Provider name to use for the request (e.g., 'openai', 'mistral'). When provided, the model parameter should contain only the model name.</p> <code>None</code> <code>messages</code> <code>list[dict[str, Any] | ChatCompletionMessage]</code> <p>List of messages for the conversation</p> required <code>tools</code> <code>list[dict[str, Any] | Callable[..., Any]] | None</code> <p>List of tools for tool calling. Can be Python callables or OpenAI tool format dicts</p> <code>None</code> <code>tool_choice</code> <code>str | dict[str, Any] | None</code> <p>Controls which tools the model can call</p> <code>None</code> <code>temperature</code> <code>float | None</code> <p>Controls randomness in the response (0.0 to 2.0)</p> <code>None</code> <code>top_p</code> <code>float | None</code> <p>Controls diversity via nucleus sampling (0.0 to 1.0)</p> <code>None</code> <code>max_tokens</code> <code>int | None</code> <p>Maximum number of tokens to generate</p> <code>None</code> <code>response_format</code> <code>dict[str, Any] | type[BaseModel] | None</code> <p>Format specification for the response</p> <code>None</code> <code>stream</code> <code>bool | None</code> <p>Whether to stream the response</p> <code>None</code> <code>n</code> <code>int | None</code> <p>Number of completions to generate</p> <code>None</code> <code>stop</code> <code>str | list[str] | None</code> <p>Stop sequences for generation</p> <code>None</code> <code>presence_penalty</code> <code>float | None</code> <p>Penalize new tokens based on presence in text</p> <code>None</code> <code>frequency_penalty</code> <code>float | None</code> <p>Penalize new tokens based on frequency in text</p> <code>None</code> <code>seed</code> <code>int | None</code> <p>Random seed for reproducible results</p> <code>None</code> <code>api_key</code> <code>str | None</code> <p>API key for the provider</p> <code>None</code> <code>api_base</code> <code>str | None</code> <p>Base URL for the provider API</p> <code>None</code> <code>user</code> <code>str | None</code> <p>Unique identifier for the end user</p> <code>None</code> <code>parallel_tool_calls</code> <code>bool | None</code> <p>Whether to allow parallel tool calls</p> <code>None</code> <code>logprobs</code> <code>bool | None</code> <p>Include token-level log probabilities in the response</p> <code>None</code> <code>top_logprobs</code> <code>int | None</code> <p>Number of alternatives to return when logprobs are requested</p> <code>None</code> <code>logit_bias</code> <code>dict[str, float] | None</code> <p>Bias the likelihood of specified tokens during generation</p> <code>None</code> <code>stream_options</code> <code>dict[str, Any] | None</code> <p>Additional options controlling streaming behavior</p> <code>None</code> <code>max_completion_tokens</code> <code>int | None</code> <p>Maximum number of tokens for the completion</p> <code>None</code> <code>reasoning_effort</code> <code>Literal['minimal', 'low', 'medium', 'high', 'auto'] | None</code> <p>Reasoning effort level for models that support it. \"auto\" will map to each provider's default.</p> <code>'auto'</code> <code>client_args</code> <code>dict[str, Any] | None</code> <p>Additional provider-specific arguments that will be passed to the provider's client instantiation.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional provider-specific arguments that will be passed to the provider's API call.</p> <code>{}</code> <p>Returns:</p> Type Description <code>ChatCompletion | AsyncIterator[ChatCompletionChunk]</code> <p>The completion response from the provider</p> Source code in <code>src/any_llm/api.py</code> <pre><code>async def acompletion(\n    model: str,\n    messages: list[dict[str, Any] | ChatCompletionMessage],\n    *,\n    provider: str | LLMProvider | None = None,\n    tools: list[dict[str, Any] | Callable[..., Any]] | None = None,\n    tool_choice: str | dict[str, Any] | None = None,\n    temperature: float | None = None,\n    top_p: float | None = None,\n    max_tokens: int | None = None,\n    response_format: dict[str, Any] | type[BaseModel] | None = None,\n    stream: bool | None = None,\n    n: int | None = None,\n    stop: str | list[str] | None = None,\n    presence_penalty: float | None = None,\n    frequency_penalty: float | None = None,\n    seed: int | None = None,\n    api_key: str | None = None,\n    api_base: str | None = None,\n    user: str | None = None,\n    parallel_tool_calls: bool | None = None,\n    logprobs: bool | None = None,\n    top_logprobs: int | None = None,\n    logit_bias: dict[str, float] | None = None,\n    stream_options: dict[str, Any] | None = None,\n    max_completion_tokens: int | None = None,\n    reasoning_effort: Literal[\"minimal\", \"low\", \"medium\", \"high\", \"auto\"] | None = \"auto\",\n    client_args: dict[str, Any] | None = None,\n    **kwargs: Any,\n) -&gt; ChatCompletion | AsyncIterator[ChatCompletionChunk]:\n    \"\"\"Create a chat completion asynchronously.\n\n    Args:\n        model: Model identifier. **Recommended**: Use with separate `provider` parameter (e.g., model='gpt-4', provider='openai').\n            **Alternative**: Combined format 'provider:model' (e.g., 'openai:gpt-4').\n            Legacy format 'provider/model' is also supported but deprecated.\n        provider: **Recommended**: Provider name to use for the request (e.g., 'openai', 'mistral').\n            When provided, the model parameter should contain only the model name.\n        messages: List of messages for the conversation\n        tools: List of tools for tool calling. Can be Python callables or OpenAI tool format dicts\n        tool_choice: Controls which tools the model can call\n        temperature: Controls randomness in the response (0.0 to 2.0)\n        top_p: Controls diversity via nucleus sampling (0.0 to 1.0)\n        max_tokens: Maximum number of tokens to generate\n        response_format: Format specification for the response\n        stream: Whether to stream the response\n        n: Number of completions to generate\n        stop: Stop sequences for generation\n        presence_penalty: Penalize new tokens based on presence in text\n        frequency_penalty: Penalize new tokens based on frequency in text\n        seed: Random seed for reproducible results\n        api_key: API key for the provider\n        api_base: Base URL for the provider API\n        user: Unique identifier for the end user\n        parallel_tool_calls: Whether to allow parallel tool calls\n        logprobs: Include token-level log probabilities in the response\n        top_logprobs: Number of alternatives to return when logprobs are requested\n        logit_bias: Bias the likelihood of specified tokens during generation\n        stream_options: Additional options controlling streaming behavior\n        max_completion_tokens: Maximum number of tokens for the completion\n        reasoning_effort: Reasoning effort level for models that support it. \"auto\" will map to each provider's default.\n        client_args: Additional provider-specific arguments that will be passed to the provider's client instantiation.\n        **kwargs: Additional provider-specific arguments that will be passed to the provider's API call.\n\n    Returns:\n        The completion response from the provider\n\n    \"\"\"\n    all_args = locals()\n    all_args.pop(\"provider\")\n    kwargs = all_args.pop(\"kwargs\")\n\n    model = all_args.pop(\"model\")\n    if provider is None:\n        provider_key, model_id = AnyLLM.split_model_provider(model)\n    else:\n        provider_key = LLMProvider.from_string(provider)\n        model_id = model\n    all_args[\"model\"] = model_id\n\n    llm = AnyLLM.create(\n        provider_key,\n        api_key=all_args.pop(\"api_key\"),\n        api_base=all_args.pop(\"api_base\"),\n        **all_args.pop(\"client_args\") or {},\n    )\n    return await llm.acompletion(**all_args, **kwargs)\n</code></pre>"},{"location":"api/embedding/","title":"Embedding","text":""},{"location":"api/embedding/#embedding","title":"Embedding","text":""},{"location":"api/embedding/#any_llm.api.embedding","title":"<code>any_llm.api.embedding(model, inputs, *, provider=None, api_key=None, api_base=None, client_args=None, **kwargs)</code>","text":"<p>Create an embedding.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Model identifier. Recommended: Use with separate <code>provider</code> parameter (e.g., model='gpt-4', provider='openai'). Alternative: Combined format 'provider:model' (e.g., 'openai:gpt-4'). Legacy format 'provider/model' is also supported but deprecated.</p> required <code>provider</code> <code>str | LLMProvider | None</code> <p>Recommended: Provider name to use for the request (e.g., 'openai', 'mistral'). When provided, the model parameter should contain only the model name.</p> <code>None</code> <code>inputs</code> <code>str | list[str]</code> <p>The input text to embed</p> required <code>api_key</code> <code>str | None</code> <p>API key for the provider</p> <code>None</code> <code>api_base</code> <code>str | None</code> <p>Base URL for the provider API</p> <code>None</code> <code>client_args</code> <code>dict[str, Any] | None</code> <p>Additional provider-specific arguments that will be passed to the provider's client instantiation.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional provider-specific arguments that will be passed to the provider's API call.</p> <code>{}</code> <p>Returns:</p> Type Description <code>CreateEmbeddingResponse</code> <p>The embedding of the input text</p> Source code in <code>src/any_llm/api.py</code> <pre><code>def embedding(\n    model: str,\n    inputs: str | list[str],\n    *,\n    provider: str | LLMProvider | None = None,\n    api_key: str | None = None,\n    api_base: str | None = None,\n    client_args: dict[str, Any] | None = None,\n    **kwargs: Any,\n) -&gt; CreateEmbeddingResponse:\n    \"\"\"Create an embedding.\n\n    Args:\n        model: Model identifier. **Recommended**: Use with separate `provider` parameter (e.g., model='gpt-4', provider='openai').\n            **Alternative**: Combined format 'provider:model' (e.g., 'openai:gpt-4').\n            Legacy format 'provider/model' is also supported but deprecated.\n        provider: **Recommended**: Provider name to use for the request (e.g., 'openai', 'mistral').\n            When provided, the model parameter should contain only the model name.\n        inputs: The input text to embed\n        api_key: API key for the provider\n        api_base: Base URL for the provider API\n        client_args: Additional provider-specific arguments that will be passed to the provider's client instantiation.\n        **kwargs: Additional provider-specific arguments that will be passed to the provider's API call.\n\n    Returns:\n        The embedding of the input text\n\n    \"\"\"\n    if provider is None:\n        provider_key, model_name = AnyLLM.split_model_provider(model)\n    else:\n        provider_key = LLMProvider.from_string(provider)\n        model_name = model\n\n    llm = AnyLLM.create(provider_key, api_key=api_key, api_base=api_base, **client_args or {})\n    return llm._embedding(model_name, inputs, **kwargs)\n</code></pre>"},{"location":"api/embedding/#any_llm.api.aembedding","title":"<code>any_llm.api.aembedding(model, inputs, *, provider=None, api_key=None, api_base=None, client_args=None, **kwargs)</code>  <code>async</code>","text":"<p>Create an embedding asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Model identifier in format 'provider/model' (e.g., 'openai/text-embedding-3-small'). If provider is provided, we assume that the model does not contain the provider name. Otherwise, we assume that the model contains the provider name, like 'openai/gpt-4o'.</p> required <code>provider</code> <code>str | LLMProvider | None</code> <p>Provider name to use for the request. If provided, we assume that the model does not contain the provider name. Otherwise, we assume that the model contains the provider name, like 'openai:gpt-4o'.</p> <code>None</code> <code>inputs</code> <code>str | list[str]</code> <p>The input text to embed</p> required <code>api_key</code> <code>str | None</code> <p>API key for the provider</p> <code>None</code> <code>api_base</code> <code>str | None</code> <p>Base URL for the provider API</p> <code>None</code> <code>client_args</code> <code>dict[str, Any] | None</code> <p>Additional provider-specific arguments that will be passed to the provider's client instantiation.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional provider-specific arguments that will be passed to the provider's API call.</p> <code>{}</code> <p>Returns:</p> Type Description <code>CreateEmbeddingResponse</code> <p>The embedding of the input text</p> Source code in <code>src/any_llm/api.py</code> <pre><code>async def aembedding(\n    model: str,\n    inputs: str | list[str],\n    *,\n    provider: str | LLMProvider | None = None,\n    api_key: str | None = None,\n    api_base: str | None = None,\n    client_args: dict[str, Any] | None = None,\n    **kwargs: Any,\n) -&gt; CreateEmbeddingResponse:\n    \"\"\"Create an embedding asynchronously.\n\n    Args:\n        model: Model identifier in format 'provider/model' (e.g., 'openai/text-embedding-3-small'). If provider is provided, we assume that the model does not contain the provider name. Otherwise, we assume that the model contains the provider name, like 'openai/gpt-4o'.\n        provider: Provider name to use for the request. If provided, we assume that the model does not contain the provider name. Otherwise, we assume that the model contains the provider name, like 'openai:gpt-4o'.\n        inputs: The input text to embed\n        api_key: API key for the provider\n        api_base: Base URL for the provider API\n        client_args: Additional provider-specific arguments that will be passed to the provider's client instantiation.\n        **kwargs: Additional provider-specific arguments that will be passed to the provider's API call.\n\n    Returns:\n        The embedding of the input text\n\n    \"\"\"\n    if provider is None:\n        provider_key, model_name = AnyLLM.split_model_provider(model)\n    else:\n        provider_key = LLMProvider.from_string(provider)\n        model_name = model\n\n    llm = AnyLLM.create(provider_key, api_key=api_key, api_base=api_base, **client_args or {})\n    return await llm._aembedding(model_name, inputs, **kwargs)\n</code></pre>"},{"location":"api/exceptions/","title":"Exceptions","text":""},{"location":"api/exceptions/#exceptions","title":"Exceptions","text":""},{"location":"api/exceptions/#any_llm.exceptions","title":"<code>any_llm.exceptions</code>","text":"<p>Custom exceptions for any-llm package.</p>"},{"location":"api/exceptions/#any_llm.exceptions.MissingApiKeyError","title":"<code>MissingApiKeyError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Exception raised when an API key is missing or not provided.</p> Source code in <code>src/any_llm/exceptions.py</code> <pre><code>class MissingApiKeyError(Exception):\n    \"\"\"Exception raised when an API key is missing or not provided.\"\"\"\n\n    def __init__(self, provider_name: str, env_var_name: str) -&gt; None:\n        \"\"\"Initialize the exception.\n\n        Args:\n            provider_name: Name of the provider (e.g., \"OpenAI\", \"Google\", \"Mistral\")\n            env_var_name: Name of the environment variable that should contain the API key\n\n        \"\"\"\n        self.provider_name = provider_name\n        self.env_var_name = env_var_name\n\n        message = (\n            f\"No {provider_name} API key provided. \"\n            f\"Please provide it in the config or set the {env_var_name} environment variable.\"\n        )\n\n        super().__init__(message)\n</code></pre>"},{"location":"api/exceptions/#any_llm.exceptions.MissingApiKeyError.__init__","title":"<code>__init__(provider_name, env_var_name)</code>","text":"<p>Initialize the exception.</p> <p>Parameters:</p> Name Type Description Default <code>provider_name</code> <code>str</code> <p>Name of the provider (e.g., \"OpenAI\", \"Google\", \"Mistral\")</p> required <code>env_var_name</code> <code>str</code> <p>Name of the environment variable that should contain the API key</p> required Source code in <code>src/any_llm/exceptions.py</code> <pre><code>def __init__(self, provider_name: str, env_var_name: str) -&gt; None:\n    \"\"\"Initialize the exception.\n\n    Args:\n        provider_name: Name of the provider (e.g., \"OpenAI\", \"Google\", \"Mistral\")\n        env_var_name: Name of the environment variable that should contain the API key\n\n    \"\"\"\n    self.provider_name = provider_name\n    self.env_var_name = env_var_name\n\n    message = (\n        f\"No {provider_name} API key provided. \"\n        f\"Please provide it in the config or set the {env_var_name} environment variable.\"\n    )\n\n    super().__init__(message)\n</code></pre>"},{"location":"api/exceptions/#any_llm.exceptions.UnsupportedParameterError","title":"<code>UnsupportedParameterError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Exception raised when an unsupported parameter is provided.</p> Source code in <code>src/any_llm/exceptions.py</code> <pre><code>class UnsupportedParameterError(Exception):\n    \"\"\"Exception raised when an unsupported parameter is provided.\"\"\"\n\n    def __init__(self, parameter_name: str, provider_name: str, additional_message: str | None = None) -&gt; None:\n        \"\"\"Initialize the exception.\n\n        Args:\n            parameter_name: Name of the parameter that was provided\n            provider_name: Name of the provider that does not support the parameter\n            additional_message: Optional additional information about the error.\n\n        \"\"\"\n        self.parameter_name = parameter_name\n        self.provider_name = provider_name\n\n        message = f\"'{parameter_name}' is not supported for {provider_name}\"\n        if additional_message is not None:\n            message = f\"{message}.\\n{additional_message}\"\n\n        super().__init__(message)\n</code></pre>"},{"location":"api/exceptions/#any_llm.exceptions.UnsupportedParameterError.__init__","title":"<code>__init__(parameter_name, provider_name, additional_message=None)</code>","text":"<p>Initialize the exception.</p> <p>Parameters:</p> Name Type Description Default <code>parameter_name</code> <code>str</code> <p>Name of the parameter that was provided</p> required <code>provider_name</code> <code>str</code> <p>Name of the provider that does not support the parameter</p> required <code>additional_message</code> <code>str | None</code> <p>Optional additional information about the error.</p> <code>None</code> Source code in <code>src/any_llm/exceptions.py</code> <pre><code>def __init__(self, parameter_name: str, provider_name: str, additional_message: str | None = None) -&gt; None:\n    \"\"\"Initialize the exception.\n\n    Args:\n        parameter_name: Name of the parameter that was provided\n        provider_name: Name of the provider that does not support the parameter\n        additional_message: Optional additional information about the error.\n\n    \"\"\"\n    self.parameter_name = parameter_name\n    self.provider_name = provider_name\n\n    message = f\"'{parameter_name}' is not supported for {provider_name}\"\n    if additional_message is not None:\n        message = f\"{message}.\\n{additional_message}\"\n\n    super().__init__(message)\n</code></pre>"},{"location":"api/exceptions/#any_llm.exceptions.UnsupportedProviderError","title":"<code>UnsupportedProviderError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Exception raised when an unsupported provider is requested.</p> Source code in <code>src/any_llm/exceptions.py</code> <pre><code>class UnsupportedProviderError(Exception):\n    \"\"\"Exception raised when an unsupported provider is requested.\"\"\"\n\n    def __init__(self, provider_key: str, supported_providers: list[str]) -&gt; None:\n        \"\"\"Initialize the exception.\n\n        Args:\n            provider_key: The provider key that was requested\n            supported_providers: List of supported provider keys\n\n        \"\"\"\n        self.provider_key = provider_key\n        self.supported_providers = supported_providers\n\n        message = f\"'{provider_key}' is not a supported provider. Supported providers: {', '.join(supported_providers)}\"\n\n        super().__init__(message)\n</code></pre>"},{"location":"api/exceptions/#any_llm.exceptions.UnsupportedProviderError.__init__","title":"<code>__init__(provider_key, supported_providers)</code>","text":"<p>Initialize the exception.</p> <p>Parameters:</p> Name Type Description Default <code>provider_key</code> <code>str</code> <p>The provider key that was requested</p> required <code>supported_providers</code> <code>list[str]</code> <p>List of supported provider keys</p> required Source code in <code>src/any_llm/exceptions.py</code> <pre><code>def __init__(self, provider_key: str, supported_providers: list[str]) -&gt; None:\n    \"\"\"Initialize the exception.\n\n    Args:\n        provider_key: The provider key that was requested\n        supported_providers: List of supported provider keys\n\n    \"\"\"\n    self.provider_key = provider_key\n    self.supported_providers = supported_providers\n\n    message = f\"'{provider_key}' is not a supported provider. Supported providers: {', '.join(supported_providers)}\"\n\n    super().__init__(message)\n</code></pre>"},{"location":"api/list_models/","title":"List Models","text":""},{"location":"api/list_models/#models","title":"Models","text":""},{"location":"api/list_models/#any_llm.api.list_models","title":"<code>any_llm.api.list_models(provider, api_key=None, api_base=None, client_args=None, **kwargs)</code>","text":"<p>List available models for a provider.</p> Source code in <code>src/any_llm/api.py</code> <pre><code>def list_models(\n    provider: str | LLMProvider,\n    api_key: str | None = None,\n    api_base: str | None = None,\n    client_args: dict[str, Any] | None = None,\n    **kwargs: Any,\n) -&gt; Sequence[Model]:\n    \"\"\"List available models for a provider.\"\"\"\n    llm = AnyLLM.create(LLMProvider.from_string(provider), api_key=api_key, api_base=api_base, **client_args or {})\n    return llm.list_models(**kwargs)\n</code></pre>"},{"location":"api/list_models/#any_llm.api.alist_models","title":"<code>any_llm.api.alist_models(provider, api_key=None, api_base=None, client_args=None, **kwargs)</code>  <code>async</code>","text":"<p>List available models for a provider asynchronously.</p> Source code in <code>src/any_llm/api.py</code> <pre><code>async def alist_models(\n    provider: str | LLMProvider,\n    api_key: str | None = None,\n    api_base: str | None = None,\n    client_args: dict[str, Any] | None = None,\n    **kwargs: Any,\n) -&gt; Sequence[Model]:\n    \"\"\"List available models for a provider asynchronously.\"\"\"\n    llm = AnyLLM.create(LLMProvider.from_string(provider), api_key=api_key, api_base=api_base, **client_args or {})\n    return await llm.alist_models(**kwargs)\n</code></pre>"},{"location":"api/responses/","title":"Responses","text":""},{"location":"api/responses/#responses","title":"Responses","text":"<p>Warning</p> <p>This API is experimental and subject to changes based upon our experience as we integrate additional providers. Use with caution.</p>"},{"location":"api/responses/#any_llm.api.responses","title":"<code>any_llm.api.responses(model, input_data, *, provider=None, tools=None, tool_choice=None, max_output_tokens=None, temperature=None, top_p=None, stream=None, api_key=None, api_base=None, instructions=None, max_tool_calls=None, parallel_tool_calls=None, reasoning=None, text=None, client_args=None, **kwargs)</code>","text":"<p>Create a response using the OpenAI-style Responses API.</p> <p>This follows the OpenAI Responses API shape and returns the aliased <code>any_llm.types.responses.Response</code> type. If <code>stream=True</code>, an iterator of <code>any_llm.types.responses.ResponseStreamEvent</code> items is returned.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Model identifier in format 'provider/model' (e.g., 'openai/gpt-4o'). If provider is provided, we assume that the model does not contain the provider name. Otherwise, we assume that the model contains the provider name, like 'openai/gpt-4o'.</p> required <code>provider</code> <code>str | LLMProvider | None</code> <p>Provider name to use for the request. If provided, we assume that the model does not contain the provider name. Otherwise, we assume that the model contains the provider name, like 'openai:gpt-4o'.</p> <code>None</code> <code>input_data</code> <code>str | ResponseInputParam</code> <p>The input payload accepted by provider's Responses API. For OpenAI-compatible providers, this is typically a list mixing text, images, and tool instructions, or a dict per OpenAI spec.</p> required <code>tools</code> <code>list[dict[str, Any] | Callable[..., Any]] | None</code> <p>Optional tools for tool calling (Python callables or OpenAI tool dicts)</p> <code>None</code> <code>tool_choice</code> <code>str | dict[str, Any] | None</code> <p>Controls which tools the model can call</p> <code>None</code> <code>max_output_tokens</code> <code>int | None</code> <p>Maximum number of output tokens to generate</p> <code>None</code> <code>temperature</code> <code>float | None</code> <p>Controls randomness in the response (0.0 to 2.0)</p> <code>None</code> <code>top_p</code> <code>float | None</code> <p>Controls diversity via nucleus sampling (0.0 to 1.0)</p> <code>None</code> <code>stream</code> <code>bool | None</code> <p>Whether to stream response events</p> <code>None</code> <code>api_key</code> <code>str | None</code> <p>API key for the provider</p> <code>None</code> <code>api_base</code> <code>str | None</code> <p>Base URL for the provider API</p> <code>None</code> <code>instructions</code> <code>str | None</code> <p>A system (or developer) message inserted into the model's context.</p> <code>None</code> <code>max_tool_calls</code> <code>int | None</code> <p>The maximum number of total calls to built-in tools that can be processed in a response. This maximum number applies across all built-in tool calls, not per individual tool. Any further attempts to call a tool by the model will be ignored.</p> <code>None</code> <code>parallel_tool_calls</code> <code>int | None</code> <p>Whether to allow the model to run tool calls in parallel.</p> <code>None</code> <code>reasoning</code> <code>Any | None</code> <p>Configuration options for reasoning models.</p> <code>None</code> <code>text</code> <code>Any | None</code> <p>Configuration options for a text response from the model. Can be plain text or structured JSON data.</p> <code>None</code> <code>client_args</code> <code>dict[str, Any] | None</code> <p>Additional provider-specific arguments that will be passed to the provider's client instantiation.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional provider-specific arguments that will be passed to the provider's API call.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Response | Iterator[ResponseStreamEvent]</code> <p>Either a <code>Response</code> object (non-streaming) or an iterator of</p> <code>Response | Iterator[ResponseStreamEvent]</code> <p><code>ResponseStreamEvent</code> (streaming).</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the selected provider does not support the Responses API.</p> Source code in <code>src/any_llm/api.py</code> <pre><code>def responses(\n    model: str,\n    input_data: str | ResponseInputParam,\n    *,\n    provider: str | LLMProvider | None = None,\n    tools: list[dict[str, Any] | Callable[..., Any]] | None = None,\n    tool_choice: str | dict[str, Any] | None = None,\n    max_output_tokens: int | None = None,\n    temperature: float | None = None,\n    top_p: float | None = None,\n    stream: bool | None = None,\n    api_key: str | None = None,\n    api_base: str | None = None,\n    instructions: str | None = None,\n    max_tool_calls: int | None = None,\n    parallel_tool_calls: int | None = None,\n    reasoning: Any | None = None,\n    text: Any | None = None,\n    client_args: dict[str, Any] | None = None,\n    **kwargs: Any,\n) -&gt; Response | Iterator[ResponseStreamEvent]:\n    \"\"\"Create a response using the OpenAI-style Responses API.\n\n    This follows the OpenAI Responses API shape and returns the aliased\n    `any_llm.types.responses.Response` type. If `stream=True`, an iterator of\n    `any_llm.types.responses.ResponseStreamEvent` items is returned.\n\n    Args:\n        model: Model identifier in format 'provider/model' (e.g., 'openai/gpt-4o'). If provider is provided, we assume that the model does not contain the provider name. Otherwise, we assume that the model contains the provider name, like 'openai/gpt-4o'.\n        provider: Provider name to use for the request. If provided, we assume that the model does not contain the provider name. Otherwise, we assume that the model contains the provider name, like 'openai:gpt-4o'.\n        input_data: The input payload accepted by provider's Responses API.\n            For OpenAI-compatible providers, this is typically a list mixing\n            text, images, and tool instructions, or a dict per OpenAI spec.\n        tools: Optional tools for tool calling (Python callables or OpenAI tool dicts)\n        tool_choice: Controls which tools the model can call\n        max_output_tokens: Maximum number of output tokens to generate\n        temperature: Controls randomness in the response (0.0 to 2.0)\n        top_p: Controls diversity via nucleus sampling (0.0 to 1.0)\n        stream: Whether to stream response events\n        api_key: API key for the provider\n        api_base: Base URL for the provider API\n        instructions: A system (or developer) message inserted into the model's context.\n        max_tool_calls: The maximum number of total calls to built-in tools that can be processed in a response. This maximum number applies across all built-in tool calls, not per individual tool. Any further attempts to call a tool by the model will be ignored.\n        parallel_tool_calls: Whether to allow the model to run tool calls in parallel.\n        reasoning: Configuration options for reasoning models.\n        text: Configuration options for a text response from the model. Can be plain text or structured JSON data.\n        client_args: Additional provider-specific arguments that will be passed to the provider's client instantiation.\n        **kwargs: Additional provider-specific arguments that will be passed to the provider's API call.\n\n    Returns:\n        Either a `Response` object (non-streaming) or an iterator of\n        `ResponseStreamEvent` (streaming).\n\n    Raises:\n        NotImplementedError: If the selected provider does not support the Responses API.\n\n    \"\"\"\n    all_args = locals()\n    all_args.pop(\"provider\")\n    kwargs = all_args.pop(\"kwargs\")\n\n    model = all_args.pop(\"model\")\n    if provider is None:\n        provider_key, model_id = AnyLLM.split_model_provider(model)\n    else:\n        provider_key = LLMProvider.from_string(provider)\n        model_id = model\n    all_args[\"model\"] = model_id\n\n    llm = AnyLLM.create(\n        provider_key,\n        api_key=all_args.pop(\"api_key\"),\n        api_base=all_args.pop(\"api_base\"),\n        **all_args.pop(\"client_args\") or {},\n    )\n    return llm.responses(**all_args, **kwargs)\n</code></pre>"},{"location":"api/responses/#any_llm.api.aresponses","title":"<code>any_llm.api.aresponses(model, input_data, *, provider=None, tools=None, tool_choice=None, max_output_tokens=None, temperature=None, top_p=None, stream=None, api_key=None, api_base=None, instructions=None, max_tool_calls=None, parallel_tool_calls=None, reasoning=None, text=None, client_args=None, **kwargs)</code>  <code>async</code>","text":"<p>Create a response using the OpenAI-style Responses API.</p> <p>This follows the OpenAI Responses API shape and returns the aliased <code>any_llm.types.responses.Response</code> type. If <code>stream=True</code>, an iterator of <code>any_llm.types.responses.ResponseStreamEvent</code> items is returned.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>Model identifier in format 'provider/model' (e.g., 'openai/gpt-4o'). If provider is provided, we assume that the model does not contain the provider name. Otherwise, we assume that the model contains the provider name, like 'openai/gpt-4o'.</p> required <code>provider</code> <code>str | LLMProvider | None</code> <p>Provider name to use for the request. If provided, we assume that the model does not contain the provider name. Otherwise, we assume that the model contains the provider name, like 'openai:gpt-4o'.</p> <code>None</code> <code>input_data</code> <code>str | ResponseInputParam</code> <p>The input payload accepted by provider's Responses API. For OpenAI-compatible providers, this is typically a list mixing text, images, and tool instructions, or a dict per OpenAI spec.</p> required <code>tools</code> <code>list[dict[str, Any] | Callable[..., Any]] | None</code> <p>Optional tools for tool calling (Python callables or OpenAI tool dicts)</p> <code>None</code> <code>tool_choice</code> <code>str | dict[str, Any] | None</code> <p>Controls which tools the model can call</p> <code>None</code> <code>max_output_tokens</code> <code>int | None</code> <p>Maximum number of output tokens to generate</p> <code>None</code> <code>temperature</code> <code>float | None</code> <p>Controls randomness in the response (0.0 to 2.0)</p> <code>None</code> <code>top_p</code> <code>float | None</code> <p>Controls diversity via nucleus sampling (0.0 to 1.0)</p> <code>None</code> <code>stream</code> <code>bool | None</code> <p>Whether to stream response events</p> <code>None</code> <code>api_key</code> <code>str | None</code> <p>API key for the provider</p> <code>None</code> <code>api_base</code> <code>str | None</code> <p>Base URL for the provider API</p> <code>None</code> <code>instructions</code> <code>str | None</code> <p>A system (or developer) message inserted into the model's context.</p> <code>None</code> <code>max_tool_calls</code> <code>int | None</code> <p>The maximum number of total calls to built-in tools that can be processed in a response. This maximum number applies across all built-in tool calls, not per individual tool. Any further attempts to call a tool by the model will be ignored.</p> <code>None</code> <code>parallel_tool_calls</code> <code>int | None</code> <p>Whether to allow the model to run tool calls in parallel.</p> <code>None</code> <code>reasoning</code> <code>Any | None</code> <p>Configuration options for reasoning models.</p> <code>None</code> <code>text</code> <code>Any | None</code> <p>Configuration options for a text response from the model. Can be plain text or structured JSON data.</p> <code>None</code> <code>client_args</code> <code>dict[str, Any] | None</code> <p>Additional provider-specific arguments that will be passed to the provider's client instantiation.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional provider-specific arguments that will be passed to the provider's API call.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Response | AsyncIterator[ResponseStreamEvent]</code> <p>Either a <code>Response</code> object (non-streaming) or an iterator of</p> <code>Response | AsyncIterator[ResponseStreamEvent]</code> <p><code>ResponseStreamEvent</code> (streaming).</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>If the selected provider does not support the Responses API.</p> Source code in <code>src/any_llm/api.py</code> <pre><code>async def aresponses(\n    model: str,\n    input_data: str | ResponseInputParam,\n    *,\n    provider: str | LLMProvider | None = None,\n    tools: list[dict[str, Any] | Callable[..., Any]] | None = None,\n    tool_choice: str | dict[str, Any] | None = None,\n    max_output_tokens: int | None = None,\n    temperature: float | None = None,\n    top_p: float | None = None,\n    stream: bool | None = None,\n    api_key: str | None = None,\n    api_base: str | None = None,\n    instructions: str | None = None,\n    max_tool_calls: int | None = None,\n    parallel_tool_calls: int | None = None,\n    reasoning: Any | None = None,\n    text: Any | None = None,\n    client_args: dict[str, Any] | None = None,\n    **kwargs: Any,\n) -&gt; Response | AsyncIterator[ResponseStreamEvent]:\n    \"\"\"Create a response using the OpenAI-style Responses API.\n\n    This follows the OpenAI Responses API shape and returns the aliased\n    `any_llm.types.responses.Response` type. If `stream=True`, an iterator of\n    `any_llm.types.responses.ResponseStreamEvent` items is returned.\n\n    Args:\n        model: Model identifier in format 'provider/model' (e.g., 'openai/gpt-4o'). If provider is provided, we assume that the model does not contain the provider name. Otherwise, we assume that the model contains the provider name, like 'openai/gpt-4o'.\n        provider: Provider name to use for the request. If provided, we assume that the model does not contain the provider name. Otherwise, we assume that the model contains the provider name, like 'openai:gpt-4o'.\n        input_data: The input payload accepted by provider's Responses API.\n            For OpenAI-compatible providers, this is typically a list mixing\n            text, images, and tool instructions, or a dict per OpenAI spec.\n        tools: Optional tools for tool calling (Python callables or OpenAI tool dicts)\n        tool_choice: Controls which tools the model can call\n        max_output_tokens: Maximum number of output tokens to generate\n        temperature: Controls randomness in the response (0.0 to 2.0)\n        top_p: Controls diversity via nucleus sampling (0.0 to 1.0)\n        stream: Whether to stream response events\n        api_key: API key for the provider\n        api_base: Base URL for the provider API\n        instructions: A system (or developer) message inserted into the model's context.\n        max_tool_calls: The maximum number of total calls to built-in tools that can be processed in a response. This maximum number applies across all built-in tool calls, not per individual tool. Any further attempts to call a tool by the model will be ignored.\n        parallel_tool_calls: Whether to allow the model to run tool calls in parallel.\n        reasoning: Configuration options for reasoning models.\n        text: Configuration options for a text response from the model. Can be plain text or structured JSON data.\n        client_args: Additional provider-specific arguments that will be passed to the provider's client instantiation.\n        **kwargs: Additional provider-specific arguments that will be passed to the provider's API call.\n\n    Returns:\n        Either a `Response` object (non-streaming) or an iterator of\n        `ResponseStreamEvent` (streaming).\n\n    Raises:\n        NotImplementedError: If the selected provider does not support the Responses API.\n\n    \"\"\"\n    all_args = locals()\n    all_args.pop(\"provider\")\n    kwargs = all_args.pop(\"kwargs\")\n\n    model = all_args.pop(\"model\")\n    if provider is None:\n        provider_key, model_id = AnyLLM.split_model_provider(model)\n    else:\n        provider_key = LLMProvider.from_string(provider)\n        model_id = model\n    all_args[\"model\"] = model_id\n\n    llm = AnyLLM.create(\n        provider_key,\n        api_key=all_args.pop(\"api_key\"),\n        api_base=all_args.pop(\"api_base\"),\n        **all_args.pop(\"client_args\") or {},\n    )\n    return await llm.aresponses(**all_args, **kwargs)\n</code></pre>"},{"location":"api/types/batch/","title":"Batch","text":""},{"location":"api/types/batch/#batch-types","title":"Batch Types","text":"<p>Data models and types for batch operations.</p>"},{"location":"api/types/batch/#any_llm.types.batch","title":"<code>any_llm.types.batch</code>","text":""},{"location":"api/types/completion/","title":"Completion","text":""},{"location":"api/types/completion/#completion-types","title":"Completion Types","text":"<p>Data models and types for completion operations.</p>"},{"location":"api/types/completion/#any_llm.types.completion","title":"<code>any_llm.types.completion</code>","text":""},{"location":"api/types/completion/#any_llm.types.completion.CompletionParams","title":"<code>CompletionParams</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Normalized parameters for chat completions.</p> <p>This model is used internally to pass structured parameters from the public API layer to provider implementations, avoiding very long function signatures while keeping type safety.</p> Source code in <code>src/any_llm/types/completion.py</code> <pre><code>class CompletionParams(BaseModel):\n    \"\"\"Normalized parameters for chat completions.\n\n    This model is used internally to pass structured parameters from the public\n    API layer to provider implementations, avoiding very long function\n    signatures while keeping type safety.\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"forbid\")\n\n    model_id: str\n    \"\"\"Model identifier (e.g., 'mistral-small-latest')\"\"\"\n\n    messages: list[dict[str, Any]]\n    \"\"\"List of messages for the conversation\"\"\"\n\n    @field_validator(\"messages\")\n    def check_messages_not_empty(cls, v: list[dict[str, Any]]) -&gt; list[dict[str, Any]]:  # noqa: N805\n        if not v:\n            msg = \"The `messages` list cannot be empty.\"\n            raise ValueError(msg)\n        return v\n\n    tools: list[dict[str, Any] | Any] | None = None\n    \"\"\"List of tools for tool calling. Should be converted to OpenAI tool format dicts\"\"\"\n\n    tool_choice: str | dict[str, Any] | None = None\n    \"\"\"Controls which tools the model can call\"\"\"\n\n    temperature: float | None = None\n    \"\"\"Controls randomness in the response (0.0 to 2.0)\"\"\"\n\n    top_p: float | None = None\n    \"\"\"Controls diversity via nucleus sampling (0.0 to 1.0)\"\"\"\n\n    max_tokens: int | None = None\n    \"\"\"Maximum number of tokens to generate\"\"\"\n\n    response_format: dict[str, Any] | type[BaseModel] | None = None\n    \"\"\"Format specification for the response\"\"\"\n\n    stream: bool | None = None\n    \"\"\"Whether to stream the response\"\"\"\n\n    n: int | None = None\n    \"\"\"Number of completions to generate\"\"\"\n\n    stop: str | list[str] | None = None\n    \"\"\"Stop sequences for generation\"\"\"\n\n    presence_penalty: float | None = None\n    \"\"\"Penalize new tokens based on presence in text\"\"\"\n\n    frequency_penalty: float | None = None\n    \"\"\"Penalize new tokens based on frequency in text\"\"\"\n\n    seed: int | None = None\n    \"\"\"Random seed for reproducible results\"\"\"\n\n    user: str | None = None\n    \"\"\"Unique identifier for the end user\"\"\"\n\n    parallel_tool_calls: bool | None = None\n    \"\"\"Whether to allow parallel tool calls\"\"\"\n\n    logprobs: bool | None = None\n    \"\"\"Include token-level log probabilities in the response\"\"\"\n\n    top_logprobs: int | None = None\n    \"\"\"Number of top alternatives to return when logprobs are requested\"\"\"\n\n    logit_bias: dict[str, float] | None = None\n    \"\"\"Bias the likelihood of specified tokens during generation\"\"\"\n\n    stream_options: dict[str, Any] | None = None\n    \"\"\"Additional options controlling streaming behavior\"\"\"\n\n    max_completion_tokens: int | None = None\n    \"\"\"Maximum number of tokens for the completion (provider-dependent)\"\"\"\n\n    reasoning_effort: Literal[\"minimal\", \"low\", \"medium\", \"high\", \"auto\"] | None = \"auto\"\n    \"\"\"Reasoning effort level for models that support it. \"auto\" will map to each provider's default.\"\"\"\n</code></pre>"},{"location":"api/types/completion/#any_llm.types.completion.CompletionParams.frequency_penalty","title":"<code>frequency_penalty = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Penalize new tokens based on frequency in text</p>"},{"location":"api/types/completion/#any_llm.types.completion.CompletionParams.logit_bias","title":"<code>logit_bias = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Bias the likelihood of specified tokens during generation</p>"},{"location":"api/types/completion/#any_llm.types.completion.CompletionParams.logprobs","title":"<code>logprobs = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Include token-level log probabilities in the response</p>"},{"location":"api/types/completion/#any_llm.types.completion.CompletionParams.max_completion_tokens","title":"<code>max_completion_tokens = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Maximum number of tokens for the completion (provider-dependent)</p>"},{"location":"api/types/completion/#any_llm.types.completion.CompletionParams.max_tokens","title":"<code>max_tokens = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Maximum number of tokens to generate</p>"},{"location":"api/types/completion/#any_llm.types.completion.CompletionParams.messages","title":"<code>messages</code>  <code>instance-attribute</code>","text":"<p>List of messages for the conversation</p>"},{"location":"api/types/completion/#any_llm.types.completion.CompletionParams.model_id","title":"<code>model_id</code>  <code>instance-attribute</code>","text":"<p>Model identifier (e.g., 'mistral-small-latest')</p>"},{"location":"api/types/completion/#any_llm.types.completion.CompletionParams.n","title":"<code>n = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of completions to generate</p>"},{"location":"api/types/completion/#any_llm.types.completion.CompletionParams.parallel_tool_calls","title":"<code>parallel_tool_calls = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to allow parallel tool calls</p>"},{"location":"api/types/completion/#any_llm.types.completion.CompletionParams.presence_penalty","title":"<code>presence_penalty = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Penalize new tokens based on presence in text</p>"},{"location":"api/types/completion/#any_llm.types.completion.CompletionParams.reasoning_effort","title":"<code>reasoning_effort = 'auto'</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Reasoning effort level for models that support it. \"auto\" will map to each provider's default.</p>"},{"location":"api/types/completion/#any_llm.types.completion.CompletionParams.response_format","title":"<code>response_format = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Format specification for the response</p>"},{"location":"api/types/completion/#any_llm.types.completion.CompletionParams.seed","title":"<code>seed = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Random seed for reproducible results</p>"},{"location":"api/types/completion/#any_llm.types.completion.CompletionParams.stop","title":"<code>stop = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Stop sequences for generation</p>"},{"location":"api/types/completion/#any_llm.types.completion.CompletionParams.stream","title":"<code>stream = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to stream the response</p>"},{"location":"api/types/completion/#any_llm.types.completion.CompletionParams.stream_options","title":"<code>stream_options = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Additional options controlling streaming behavior</p>"},{"location":"api/types/completion/#any_llm.types.completion.CompletionParams.temperature","title":"<code>temperature = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Controls randomness in the response (0.0 to 2.0)</p>"},{"location":"api/types/completion/#any_llm.types.completion.CompletionParams.tool_choice","title":"<code>tool_choice = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Controls which tools the model can call</p>"},{"location":"api/types/completion/#any_llm.types.completion.CompletionParams.tools","title":"<code>tools = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>List of tools for tool calling. Should be converted to OpenAI tool format dicts</p>"},{"location":"api/types/completion/#any_llm.types.completion.CompletionParams.top_logprobs","title":"<code>top_logprobs = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of top alternatives to return when logprobs are requested</p>"},{"location":"api/types/completion/#any_llm.types.completion.CompletionParams.top_p","title":"<code>top_p = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Controls diversity via nucleus sampling (0.0 to 1.0)</p>"},{"location":"api/types/completion/#any_llm.types.completion.CompletionParams.user","title":"<code>user = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Unique identifier for the end user</p>"},{"location":"api/types/model/","title":"Model","text":""},{"location":"api/types/model/#model-types","title":"Model Types","text":"<p>Data models and types for model operations.</p>"},{"location":"api/types/model/#any_llm.types.model","title":"<code>any_llm.types.model</code>","text":""},{"location":"api/types/provider/","title":"Provider","text":""},{"location":"api/types/provider/#provider-types","title":"Provider Types","text":"<p>Data models and types for provider operations.</p>"},{"location":"api/types/provider/#any_llm.types.provider","title":"<code>any_llm.types.provider</code>","text":""},{"location":"api/types/responses/","title":"Responses","text":""},{"location":"api/types/responses/#response-types","title":"Response Types","text":"<p>Data models and types for API responses.</p>"},{"location":"api/types/responses/#any_llm.types.responses","title":"<code>any_llm.types.responses</code>","text":""},{"location":"api/types/responses/#any_llm.types.responses.ResponsesParams","title":"<code>ResponsesParams</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Normalized parameters for responses API.</p> <p>This model is used internally to pass structured parameters from the public API layer to provider implementations, avoiding very long function signatures while keeping type safety.</p> Source code in <code>src/any_llm/types/responses.py</code> <pre><code>class ResponsesParams(BaseModel):\n    \"\"\"Normalized parameters for responses API.\n\n    This model is used internally to pass structured parameters from the public\n    API layer to provider implementations, avoiding very long function\n    signatures while keeping type safety.\n    \"\"\"\n\n    model_config = ConfigDict(extra=\"forbid\")\n\n    model: str\n    \"\"\"Model identifier (e.g., 'mistral-small-latest')\"\"\"\n\n    input: str | ResponseInputParam\n    \"\"\"The input payload accepted by provider's Responses API.\n        For OpenAI-compatible providers, this is typically a list mixing\n        text, images, and tool instructions, or a dict per OpenAI spec.\n    \"\"\"\n\n    instructions: str | None = None\n\n    max_tool_calls: int | None = None\n\n    text: Any | None = None\n\n    tools: list[dict[str, Any]] | None = None\n    \"\"\"List of tools for tool calling. Should be converted to OpenAI tool format dicts\"\"\"\n\n    tool_choice: str | dict[str, Any] | None = None\n    \"\"\"Controls which tools the model can call\"\"\"\n\n    temperature: float | None = None\n    \"\"\"Controls randomness in the response (0.0 to 2.0)\"\"\"\n\n    top_p: float | None = None\n    \"\"\"Controls diversity via nucleus sampling (0.0 to 1.0)\"\"\"\n\n    max_output_tokens: int | None = None\n    \"\"\"Maximum number of tokens to generate\"\"\"\n\n    response_format: dict[str, Any] | type[BaseModel] | None = None\n    \"\"\"Format specification for the response\"\"\"\n\n    stream: bool | None = None\n    \"\"\"Whether to stream the response\"\"\"\n\n    parallel_tool_calls: bool | None = None\n    \"\"\"Whether to allow parallel tool calls\"\"\"\n\n    top_logprobs: int | None = None\n    \"\"\"Number of top alternatives to return when logprobs are requested\"\"\"\n\n    stream_options: dict[str, Any] | None = None\n    \"\"\"Additional options controlling streaming behavior\"\"\"\n\n    reasoning: dict[str, Any] | None = None\n    \"\"\"Configuration options for reasoning models.\"\"\"\n</code></pre>"},{"location":"api/types/responses/#any_llm.types.responses.ResponsesParams.input","title":"<code>input</code>  <code>instance-attribute</code>","text":"<p>The input payload accepted by provider's Responses API. For OpenAI-compatible providers, this is typically a list mixing text, images, and tool instructions, or a dict per OpenAI spec.</p>"},{"location":"api/types/responses/#any_llm.types.responses.ResponsesParams.max_output_tokens","title":"<code>max_output_tokens = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Maximum number of tokens to generate</p>"},{"location":"api/types/responses/#any_llm.types.responses.ResponsesParams.model","title":"<code>model</code>  <code>instance-attribute</code>","text":"<p>Model identifier (e.g., 'mistral-small-latest')</p>"},{"location":"api/types/responses/#any_llm.types.responses.ResponsesParams.parallel_tool_calls","title":"<code>parallel_tool_calls = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to allow parallel tool calls</p>"},{"location":"api/types/responses/#any_llm.types.responses.ResponsesParams.reasoning","title":"<code>reasoning = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Configuration options for reasoning models.</p>"},{"location":"api/types/responses/#any_llm.types.responses.ResponsesParams.response_format","title":"<code>response_format = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Format specification for the response</p>"},{"location":"api/types/responses/#any_llm.types.responses.ResponsesParams.stream","title":"<code>stream = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Whether to stream the response</p>"},{"location":"api/types/responses/#any_llm.types.responses.ResponsesParams.stream_options","title":"<code>stream_options = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Additional options controlling streaming behavior</p>"},{"location":"api/types/responses/#any_llm.types.responses.ResponsesParams.temperature","title":"<code>temperature = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Controls randomness in the response (0.0 to 2.0)</p>"},{"location":"api/types/responses/#any_llm.types.responses.ResponsesParams.tool_choice","title":"<code>tool_choice = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Controls which tools the model can call</p>"},{"location":"api/types/responses/#any_llm.types.responses.ResponsesParams.tools","title":"<code>tools = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>List of tools for tool calling. Should be converted to OpenAI tool format dicts</p>"},{"location":"api/types/responses/#any_llm.types.responses.ResponsesParams.top_logprobs","title":"<code>top_logprobs = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Number of top alternatives to return when logprobs are requested</p>"},{"location":"api/types/responses/#any_llm.types.responses.ResponsesParams.top_p","title":"<code>top_p = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Controls diversity via nucleus sampling (0.0 to 1.0)</p>"}]}