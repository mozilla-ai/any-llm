{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Getting Started with Any-LLM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mozilla-ai/any-llm/blob/main/docs/cookbooks/any_llm_getting_started.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "Any-LLM is a unified interface that lets you work with language models from any provider using a consistent API. Whether you're using OpenAI, Anthropic, Google, local models, or open-source alternatives, any-llm makes it easy to switch between them without changing your code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Why Any-LLM?\n",
    "- Provider Agnostic: One API for all LLM providers\n",
    "- Easy Switching: Change models with a single line\n",
    "- Cost Comparison: Compare costs across providers\n",
    "- Streaming Support: Real-time responses from any model\n",
    "- Type Safe: Full TypeScript/Python type support"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install any-llm-sdk[all] nest-asyncio -q\n",
    "\n",
    "# nest_asyncio allows us to use 'await' directly in Jupyter notebooks\n",
    "# This is needed because any-llm uses async functions for API calls\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Setting Up API Keys\n",
    "Different providers require different API keys. Let's set them up properly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "\n",
    "def setup_api_key(key_name: str, provider: str) -> None:\n",
    "    \"\"\"Set up API key for the specified provider.\"\"\"\n",
    "    if key_name not in os.environ:\n",
    "        print(f\"üîë {key_name} not found in environment\")\n",
    "        api_key = getpass(f\"Enter your {provider} API key (or press Enter to skip): \")\n",
    "        if api_key:\n",
    "            os.environ[key_name] = api_key\n",
    "            print(f\"‚úÖ {key_name} set for this session\")\n",
    "        else:\n",
    "            print(f\"‚è≠Ô∏è  Skipping {provider}\")\n",
    "    else:\n",
    "        print(f\"‚úÖ {key_name} found in environment\")\n",
    "\n",
    "\n",
    "# Set up keys for different providers\n",
    "print(\"Setting up API keys...\\n\")\n",
    "setup_api_key(\"OPENAI_API_KEY\", \"OpenAI\")\n",
    "setup_api_key(\"ANTHROPIC_API_KEY\", \"Anthropic\")\n",
    "\n",
    "#  You could add more using :\n",
    "# setup_api_key(\"GOOGLE_API_KEY\", \"Google\")\n",
    "# setup_api_key(\"MISTRAL_API_KEY\", \"Mistral\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## List Models Across Providers \n",
    "`any_llm` can list all available models for an LLM provider - in this case, we are listing out models supported by OpenAI and Anthropic. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from any_llm import AnyLLM, LLMProvider\n",
    "\n",
    "for provider in [LLMProvider.OPENAI, LLMProvider.ANTHROPIC]:\n",
    "    client = AnyLLM.create(provider=provider)\n",
    "    models = client.list_models()\n",
    "    print(f\"Provider: {provider}\")\n",
    "    print(\", \".join([model.id for model in models]))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### Expected output\n",
    "Provider: openai\n",
    "gpt-4o-mini, gpt-4-0613, gpt-4, gpt-3.5-turbo, gpt-5-search-api-2025-10-14, gpt-realtime-mini, gpt-realtime-mini-2025-10-06, sora-2, sora-2-pro, davinci-002, babbage-002, gpt-3.5-turbo-instruct, gpt-3.5-turbo-instruct-0914...\n",
    "\n",
    "Provider: anthropic\n",
    "claude-haiku-4-5-20251001, claude-sonnet-4-5-20250929, claude-opus-4-1-20250805, claude-opus-4-20250514, claude-sonnet-4-20250514, claude-3-7-sonnet-20250219, claude-3-5-haiku-20241022, claude-3-haiku-20240307\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Generate Text\n",
    "Let's use one model from each provider to generate text for the same prompt. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from any_llm import acompletion\n",
    "from any_llm.types.completion import ChatCompletion\n",
    "\n",
    "prompt = \"Write a Haiku on the solar system.\"\n",
    "\n",
    "# OpenAI\n",
    "model = \"openai:gpt-4o-mini\"\n",
    "result = await acompletion(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ],\n",
    ")\n",
    "assert isinstance(result, ChatCompletion)\n",
    "\n",
    "print(f\"Model: {result.model}\")\n",
    "print(f\"Response:\\n{result.choices[0].message.content}\\n\")\n",
    "\n",
    "# Anthropic\n",
    "model = \"anthropic:claude-haiku-4-5-20251001\"\n",
    "result = await acompletion(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "    ],\n",
    ")\n",
    "\n",
    "assert isinstance(result, ChatCompletion)\n",
    "\n",
    "print(f\"Model: {result.model}\")\n",
    "print(f\"Response:\\n{result.choices[0].message.content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "### Expected Output \n",
    "\n",
    "*Note: The haiku content will be different each time since it's generated by the LLM. This example shows the output format.*\n",
    "\n",
    "Model: gpt-4o-mini-2024-07-18\n",
    "Response:\n",
    "\n",
    "Planets spin and dance,  \n",
    "In the vast cosmic embrace,  \n",
    "Stars whisper their tales.\n",
    "\n",
    "Model: claude-haiku-4-5-20251001\n",
    "Response:\n",
    "\n",
    "Eight worlds circle round,  \n",
    "Sun's gravity holds them close‚Äî  \n",
    "Dance through endless void."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
